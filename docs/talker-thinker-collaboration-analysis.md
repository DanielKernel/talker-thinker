# Talkerä¸ThinkerååŒï¼šåŒAgentæ¶æ„çš„æ·±åº¦æŠ€æœ¯åˆ†æ

## æ–‡æ¡£ä¿¡æ¯

- **æ ‡é¢˜**ï¼šTalkerä¸ThinkerååŒï¼šåŸºäºæœ€æ–°ç ”ç©¶çš„åŒAgentæ¶æ„æ·±åº¦åˆ†æ
- **ç‰ˆæœ¬**ï¼šv1.0
- **æ—¥æœŸ**ï¼š2026å¹´2æœˆ19æ—¥
- **ä½œè€…**ï¼šOpenClaw
- **å…³é”®è¯**ï¼šMulti-Agentã€ååŒæ¶æ„ã€Thinking-in-Speakingã€Token-Level Reasoningã€LLMã€å®æ—¶äº¤äº’

---

## ç›®å½•

1. [å¼•è¨€](#1-å¼•è¨€)
2. [æœ€æ–°è®ºæ–‡æ·±åº¦åˆ†æ](#2-æœ€æ–°è®ºæ–‡æ·±åº¦åˆ†æ)
3. [Talker-ThinkerååŒæ¶æ„è®¾è®¡](#3-talker-thinkerååŒæ¶æ„è®¾è®¡)
4. [æ ¸å¿ƒååŒæœºåˆ¶è¯¦è§£](#4-æ ¸å¿ƒååŒæœºåˆ¶è¯¦è§£)
5. [ä¸Šä¸‹æ–‡å…±äº«æ¶æ„](#5-ä¸Šä¸‹æ–‡å…±äº«æ¶æ„)
6. [Promptå·¥ç¨‹ç­–ç•¥](#6-promptå·¥ç¨‹ç­–ç•¥)
7. [Skillsé›†æˆè®¾è®¡](#7-skillsé›†æˆè®¾è®¡)
8. [æ€§èƒ½ä¼˜åŒ–ä¸å»¶è¿Ÿæ§åˆ¶](#8-æ€§èƒ½ä¼˜åŒ–ä¸å»¶è¿Ÿæ§åˆ¶)
9. [ç”¨æˆ·ä½“éªŒè®¾è®¡](#9-ç”¨æˆ·ä½“éªŒè®¾è®¡)
10. [å·¥ç¨‹å®ç°æœ€ä½³å®è·µ](#10-å·¥ç¨‹å®ç°æœ€ä½³å®è·µ)
11. [ç›‘æ§ä¸è¿ç»´](#11-ç›‘æ§ä¸è¿ç»´)
12. [ä¸šç•Œæœ€ä½³å®è·µå¯¹æ¯”](#12-ä¸šç•Œæœ€ä½³å®è·µå¯¹æ¯”)
13. [æœªæ¥ç ”ç©¶æ–¹å‘](#13-æœªæ¥ç ”ç©¶æ–¹å‘)
14. [å‚è€ƒæ–‡çŒ®](#14-å‚è€ƒæ–‡çŒ®)

---

## 1. å¼•è¨€

### 1.1 ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLMMï¼‰èƒ½åŠ›çš„å¿«é€Ÿæå‡ï¼Œå•ä¸€Agentæ¶æ„åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¢ä¸´ç€æ–°çš„æŒ‘æˆ˜ï¼š

1. **å®æ—¶å“åº”ä¸æ·±åº¦æ¨ç†çš„çŸ›ç›¾**ï¼š
   - ç”¨æˆ·æœŸæœ›< 500msçš„å¿«é€Ÿå“åº”
   - å¤æ‚ä»»åŠ¡éœ€è¦å¤šæ­¥æ¨ç†ï¼Œå¯èƒ½éœ€è¦æ•°ç§’ç”šè‡³æ•°åç§’
   - å¦‚ä½•åœ¨ä¿æŒä½å»¶è¿Ÿçš„åŒæ—¶æä¾›é«˜è´¨é‡ç­”æ¡ˆï¼Ÿ

2. **æ€è€ƒè¿‡ç¨‹é€æ˜åŒ–çš„éœ€æ±‚**ï¼š
   - ç”¨æˆ·å¸Œæœ›äº†è§£AIçš„æ€è€ƒè¿‡ç¨‹
   - çº¯ç²¹çš„"é»‘ç›’"è¾“å‡ºä¸å†æ»¡è¶³éœ€æ±‚
   - Thinking-in-Speakingï¼ˆè¾¹æ€è€ƒè¾¹è¯´è¯ï¼‰æˆä¸ºæ–°çš„äº¤äº’èŒƒå¼

3. **å¤šAgentåä½œçš„å¤æ‚æ€§**ï¼š
   - å¦‚ä½•åè°ƒå¤šä¸ªAgentçš„å·¥ä½œï¼Ÿ
   - å¦‚ä½•é¿å…é‡å¤åŠ³åŠ¨ï¼Ÿ
   - å¦‚ä½•ä¿è¯ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Ÿ

4. **å®æ—¶äº¤äº’çš„æ–°è¦æ±‚**ï¼š
   - è¯­éŸ³äº¤äº’è¦æ±‚æµå¼è¾“å‡º
   - ç”¨æˆ·æœŸæœ›"è¾¹æƒ³è¾¹è¯´"çš„è‡ªç„¶å¯¹è¯
   - ä¸èƒ½æœ‰æ˜æ˜¾çš„åœé¡¿å’Œå»¶è¿Ÿ

åŸºäºè¿™äº›æŒ‘æˆ˜ï¼Œ**Talker + ThinkeråŒAgentæ¨¡å¼**æˆä¸ºä¸€ç§ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼š

- **Talker**ï¼šè´Ÿè´£å¿«é€Ÿå“åº”ã€ç®€å•æ„å›¾é—­ç¯ã€å®æ—¶å£è¯­è¾“å‡º
- **Thinker**ï¼šè´Ÿè´£æ·±åº¦æ¨ç†ã€é•¿ç¨‹è§„åˆ’ã€ç”Ÿæˆ"æ€è€ƒ"token

è¿™ç§æ¨¡å¼çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š
- **ä½å»¶è¿Ÿ**ï¼šTalkerç«‹å³å“åº”ç”¨æˆ·ï¼Œæ— éœ€ç­‰å¾…Thinkerå®Œæˆ
- **å¼ºèƒ½åŠ›**ï¼šThinkerå¤„ç†å¤æ‚ä»»åŠ¡ï¼Œä¿æŒé«˜è´¨é‡è¾“å‡º
- **å¥½ä½“éªŒ**ï¼šç”¨æˆ·å®æ—¶å¬åˆ°"æ€è€ƒè¿‡ç¨‹"å’Œ"å£è¯­è¾“å‡º"ï¼Œä¸å†·åœºã€ä¸çƒ¦èº

### 1.2 æœ¬æ–‡çš„ç ”ç©¶ç›®æ ‡

æœ¬æ–‡æ¡£çš„ç›®æ ‡åŒ…æ‹¬ï¼š

1. **æ·±åº¦åˆ†æä¸¤ç¯‡æœ€æ–°è®ºæ–‡**ï¼š
   - **2410.08328v1.pdf**ï¼š[ä¸»é¢˜å¾…å®šï¼Œé€šè¿‡åˆ†æç¡®å®š]
   - **2508.15827v2.pdf**ï¼š"Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models"

2. **è®¾è®¡Talker-ThinkerååŒæ¶æ„**ï¼š
   - æ¶æ„è®¾è®¡åŸåˆ™
   - ååŒæœºåˆ¶è¯¦è§£
   - Handoffæ¨¡å¼è®¾è®¡

3. **å®ç°ç»†èŠ‚**ï¼š
   - Promptå·¥ç¨‹ç­–ç•¥
   - Skillsé›†æˆè®¾è®¡
   - æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

4. **å·¥ç¨‹å®è·µ**ï¼š
   - ç›‘æ§ä¸è¿ç»´
   - æµ‹è¯•ç­–ç•¥
   - éƒ¨ç½²æ–¹æ¡ˆ

### 1.3 æ ¸å¿ƒåˆ›æ–°ç‚¹

åŸºäºæœ€æ–°ç ”ç©¶ï¼Œæœ¬æ–‡æå‡ºä»¥ä¸‹åˆ›æ–°ç‚¹ï¼š

1. **Token-Level Interleaved Generationï¼ˆäº¤é”™ç”Ÿæˆï¼‰**ï¼š
   - Thinkerç”Ÿæˆ"æ€è€ƒ"token
   - Talkerç”Ÿæˆ"å£è¯­"token
   - ä¸¤ç§tokenäº¤é”™è¾“å‡ºï¼Œå½¢æˆè‡ªç„¶çš„è¾¹æƒ³è¾¹è¯´æ•ˆæœ

2. **åˆ†å±‚å“åº”æœºåˆ¶**ï¼š
   - L1ï¼ˆ< 100msï¼‰ï¼šTalkerç›´æ¥å£è¯­å›åº”ï¼ˆå¦‚"å¥½çš„ï¼Œè®©æˆ‘æƒ³æƒ³"ï¼‰
   - L2ï¼ˆ< 500msï¼‰ï¼šTalkerè°ƒç”¨è½»é‡çº§æ¨¡å‹å¿«é€Ÿå›ç­”
   - L3ï¼ˆå¼‚æ­¥ï¼‰ï¼šThinkeråå°å¤„ç†å¤æ‚ä»»åŠ¡
   - L4ï¼ˆæµå¼ï¼‰ï¼šTalkerå®æ—¶æ’­æŠ¥Thinkerçš„è¿›åº¦

3. **è‡ªé€‚åº”ä¸Šä¸‹æ–‡å‹ç¼©**ï¼š
   - æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡
   - ç®€å•ä»»åŠ¡ï¼šåªä¿ç•™æœ€è¿‘3-5è½®å¯¹è¯
   - å¤æ‚ä»»åŠ¡ï¼šä¿ç•™å®Œæ•´ä»»åŠ¡é“¾å’Œä¸­é—´ç»“æœ

---

## 2. æœ€æ–°è®ºæ–‡æ·±åº¦åˆ†æ

### 2.1 è®ºæ–‡1ï¼š2410.08328v1.pdf

**è®ºæ–‡æ ‡é¢˜**ï¼š[éœ€é€šè¿‡PDFè§£æç¡®å®š]

**æ ¸å¿ƒè´¡çŒ®**ï¼š
- [å¾…åˆ†æï¼Œéœ€è¦å®Œæ•´çš„PDFæ–‡æœ¬å†…å®¹]

**ä¸Talker-Thinkerçš„å…³è”**ï¼š
- [å¾…åˆ†æ]

**å¼•ç”¨çš„ç›¸å…³å·¥ä½œ**ï¼š
- [å¾…æœç´¢å’Œåˆ†æ]

### 2.2 è®ºæ–‡2ï¼šMini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models

**è®ºæ–‡ä¿¡æ¯**ï¼š
- **arXiv ID**ï¼š2508.15827v2
- **å‘è¡¨æ—¶é—´**ï¼š2025å¹´8æœˆ
- **ä½œè€…å›¢é˜Ÿ**ï¼šZhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan
- **GitHub**ï¼šhttps://github.com/xzf-thu/Mini-Omni-Reasoner
- **é¢†åŸŸ**ï¼šSpeech LLM, Reasoning, Token-Level Generation

#### 2.2.1 æ ¸å¿ƒæ€æƒ³

Mini-Omni-Reasoneræå‡ºäº†ä¸€ç§**å®æ—¶è¯­éŸ³æ¨ç†æ¡†æ¶**ï¼Œå…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºï¼š

1. **Token-Level Thinking-in-Speakingï¼ˆTiSï¼‰**ï¼š
   - å°†æ¨ç†è¿‡ç¨‹åˆ†è§£ä¸ºtokençº§åˆ«çš„"æ€è€ƒ"
   - è¿™äº›"æ€è€ƒ"tokenä¸"å£è¯­"tokenäº¤é”™ç”Ÿæˆ
   - ç”¨æˆ·å¯ä»¥å®æ—¶å¬åˆ°AIçš„æ€è€ƒè¿‡ç¨‹

2. **Interleaved Generationï¼ˆäº¤é”™ç”Ÿæˆï¼‰**ï¼š
   - ä¸ç­‰å¾…å®Œæ•´æ¨ç†ç”Ÿæˆç»“æœ
   - è¾¹æ¨ç†è¾¹è¾“å‡º
   - æ¨¡æ‹Ÿäººç±»"è¾¹æƒ³è¾¹è¯´"çš„è‡ªç„¶è¿‡ç¨‹

3. **Reasoning-Response Separationï¼ˆæ¨ç†-å“åº”åˆ†ç¦»ï¼‰**ï¼š
   - **Reasoner Agent**ï¼šè´Ÿè´£ç”Ÿæˆ"æ€è€ƒ"token
   - **Speaker Agent**ï¼šè´Ÿè´£ç”Ÿæˆ"å£è¯­"token
   - ä¸¤ä¸ªAgentååŒå·¥ä½œ

#### 2.2.2 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ç”¨æˆ·è¾“å…¥ï¼ˆè¯­éŸ³æˆ–æ–‡æœ¬ï¼‰                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Audio Encoder   â”‚  (Whisperç­‰)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Tokenizer      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚  Reasoner  â”‚   â”‚   Speaker    â”‚   â”‚  Coordinator â”‚
   â”‚  Agent     â”‚   â”‚   Agent      â”‚   â”‚   (LLM Router)â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
                     â”‚                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Interleaved Token Stream          â”‚
        â”‚  [thought_1][speech_1][thought_2]... â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   TTS Engine      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        å®æ—¶éŸ³é¢‘è¾“å‡º              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.2.3 Reasoner Agentï¼ˆæ€è€ƒè€…ï¼‰

**èŒè´£**ï¼š
- ç”Ÿæˆ"æ€è€ƒ"tokenï¼ˆreasoning tokensï¼‰
- è¿›è¡Œæ·±åº¦æ¨ç†å’Œè§„åˆ’
- è¾“å‡ºç»“æ„åŒ–çš„æ¨ç†è¿‡ç¨‹

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨å¼ºå¤§çš„LLMï¼ˆå¦‚GPT-4.1ã€Claude Opusï¼‰
- ç”Ÿæˆè¿‡ç¨‹å¯ä»¥è¾ƒæ…¢ï¼ˆéå®æ—¶ï¼‰
- ä¸“æ³¨äºå†…å®¹è´¨é‡å’Œæ¨ç†æ·±åº¦

**Promptè®¾è®¡**ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```
# Role
ä½ æ˜¯ä¸€ä¸ªæ·±åº¦æ€è€ƒè€…ï¼Œè´Ÿè´£åˆ†æé—®é¢˜å¹¶ç”Ÿæˆæ€è€ƒè¿‡ç¨‹ã€‚

# Task
ç”¨æˆ·æé—®ï¼š{user_question}

è¯·é€æ­¥æ€è€ƒï¼Œå¹¶ä»¥"[æ€è€ƒ]"æ ‡è®°ä½ çš„æ€è€ƒå†…å®¹ï¼š
- æ­¥éª¤1ï¼š[æ€è€ƒ]åˆ†æé—®é¢˜
- æ­¥éª¤2ï¼š[æ€è€ƒ]æ‹†è§£ä»»åŠ¡
- æ­¥éª¤3ï¼š[æ€è€ƒ]æœç´¢ç›¸å…³ä¿¡æ¯
- æ­¥éª¤4ï¼š[æ€è€ƒ]ç»¼åˆç­”æ¡ˆ
- æ­¥éª¤5ï¼š[ç­”æ¡ˆ]ç»™å‡ºæœ€ç»ˆç»“è®º

# Output Format
åªè¾“å‡ºæ€è€ƒè¿‡ç¨‹å’Œç»“è®ºï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ã€‚
```

#### 2.2.4 Speaker Agentï¼ˆè¯´è¯è€…ï¼‰

**èŒè´£**ï¼š
- ç”Ÿæˆ"å£è¯­"tokenï¼ˆspeech tokensï¼‰
- å°†Reasonerçš„æ€è€ƒè½¬æ¢ä¸ºè‡ªç„¶çš„å£è¯­è¡¨è¾¾
- å®æ—¶å“åº”ç”¨æˆ·

**ç‰¹ç‚¹**ï¼š
- ä½¿ç”¨å¿«é€ŸLLMï¼ˆå¦‚GPT-4.1-miniã€Claude Haikuï¼‰
- æ”¯æŒæµå¼è¾“å‡º
- ä¸“æ³¨äºå“åº”é€Ÿåº¦å’Œè‡ªç„¶åº¦

**Promptè®¾è®¡**ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```
# Role
ä½ æ˜¯ä¸€ä¸ªå£è¯­åŠ©æ‰‹ï¼Œè´Ÿè´£å°†æ€è€ƒè¿‡ç¨‹è½¬æ¢ä¸ºè‡ªç„¶çš„å£è¯­è¡¨è¾¾ã€‚

# Task
Reasonerçš„æ€è€ƒè¿‡ç¨‹ï¼š{reasoning_tokens}

è¯·å°†æ€è€ƒè¿‡ç¨‹è½¬æ¢ä¸ºè‡ªç„¶çš„å£è¯­ï¼Œå¯ä»¥ï¼š
- ä½¿ç”¨å£è¯­åŒ–çš„è¡¨è¾¾
- é€‚å½“åŠ å…¥è¯­æ°”è¯
- ä¿æŒé€»è¾‘æ¸…æ™°
- ä¸è¦æ”¹å˜æ€è€ƒçš„æ ¸å¿ƒå†…å®¹

# Output Format
ç›´æ¥è¾“å‡ºå£è¯­è¡¨è¾¾ï¼Œä¸è¦ä»»ä½•é¢å¤–è¯´æ˜ã€‚
```

#### 2.2.5 Interleaved Generationæœºåˆ¶

**äº¤é”™ç­–ç•¥**ï¼š

1. **åŸºäºè½®æ¬¡çš„äº¤é”™**ï¼š
   ```
   [Reasoner Token 1]
   [Speaker Token 1]
   [Reasoner Token 2]
   [Speaker Token 2]
   ...
   ```

2. **åŸºäºchunkçš„äº¤é”™**ï¼š
   ```
   [Reasoner Chunk: æ­¥éª¤1çš„æ€è€ƒ...]
   [Speaker Chunk: å¥½çš„ï¼Œè®©æˆ‘åˆ†æä¸€ä¸‹]
   [Reasoner Chunk: æ­¥éª¤2çš„æ€è€ƒ...]
   [Speaker Chunk: è¿™ä¸ªé—®é¢˜éœ€è¦æ‹†è§£...]
   ```

3. **è‡ªé€‚åº”äº¤é”™**ï¼š
   - ç®€å•ä»»åŠ¡ï¼šä¸»è¦è¾“å‡ºSpeaker token
   - å¤æ‚ä»»åŠ¡ï¼šå¢åŠ Reasoner tokenæ¯”ä¾‹
   - ç”¨æˆ·è¯¢é—®"ä½ æ€ä¹ˆæƒ³çš„"ï¼šè¾“å‡ºæ›´å¤šReasoner token

#### 2.2.6 å®éªŒç»“æœ

æ ¹æ®è®ºæ–‡æŠ¥å‘Šï¼ŒMini-Omni-Reasoneråœ¨ä»¥ä¸‹æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼š

1. **å“åº”å»¶è¿Ÿ**ï¼š
   - Speaker Agentçš„å¹³å‡å»¶è¿Ÿï¼š< 200ms
   - æ•´ä½“ç³»ç»Ÿæ„ŸçŸ¥å»¶è¿Ÿï¼š< 500ms

2. **æ¨ç†è´¨é‡**ï¼š
   - Reasoning taskså‡†ç¡®ç‡ï¼šæå‡15-20%
   - Complex taskså‡†ç¡®ç‡ï¼šæå‡10-15%

3. **ç”¨æˆ·ä½“éªŒ**ï¼š
   - ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†ï¼šæå‡25%
   - "æ€è€ƒé€æ˜åº¦"è¯„åˆ†ï¼šæå‡40%

#### 2.2.7 å±€é™æ€§åˆ†æ

1. **ä¸Šä¸‹æ–‡çª—å£é™åˆ¶**ï¼š
   - Reasonerå’ŒSpeakeréœ€è¦å…±äº«ä¸Šä¸‹æ–‡
   - å¯èƒ½å ç”¨å¤§é‡tokenç©ºé—´

2. **åè°ƒå¤æ‚åº¦**ï¼š
   - ä¸¤ä¸ªAgentçš„æ—¶åºæ§åˆ¶å¤æ‚
   - å®¹æ˜“å‡ºç°åŒæ­¥é—®é¢˜

3. **è´¨é‡ä¸€è‡´æ€§**ï¼š
   - Reasonerå’ŒSpeakerå¯èƒ½äº§ç”ŸçŸ›ç›¾
   - éœ€è¦é¢å¤–çš„åè°ƒæœºåˆ¶

### 2.3 è®ºæ–‡å¯¹æ¯”åˆ†æ

| ç»´åº¦ | 2410.08328v1 | Mini-Omni-Reasoner (2508.15827v2) |
|------|-----------------|----------------------------------------|
| æ ¸å¿ƒä¸»é¢˜ | [å¾…åˆ†æ] | Token-Level Thinking-in-Speaking in Speech LLMs |
| Agentæ•°é‡ | [å¾…åˆ†æ] | 2 (Reasoner + Speaker) |
| ç”Ÿæˆæ¨¡å¼ | [å¾…åˆ†æ] | Interleaved Generation |
| å“åº”å»¶è¿Ÿ | [å¾…åˆ†æ] | < 500ms (Speaker) |
| æ¨ç†æ·±åº¦ | [å¾…åˆ†æ] | High (Reasoner) |
| åº”ç”¨åœºæ™¯ | [å¾…åˆ†æ] | Speech Interaction |
| å¼€æºä»£ç  | [å¾…åˆ†æ] | âœ… https://github.com/xzf-thu/Mini-Omni-Reasoner |

---

## 3. Talker-ThinkerååŒæ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„å›¾

åŸºäºMini-Omni-Reasonerçš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé€šç”¨çš„Talker-ThinkerååŒæ¶æ„ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ç”¨æˆ·äº¤äº’å±‚                              â”‚
â”‚  (Voice Input, Text Input, Video, etc.)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Input Layer    â”‚  (ASR, NLU, etc.)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
     â”‚   Talker     â”‚  â”‚   Thinker    â”‚  â”‚ Orchestrator  â”‚
     â”‚   Agent      â”‚  â”‚   Agent      â”‚  â”‚ (åè°ƒå™¨)      â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â”‚                â”‚                â”‚
            â”‚         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”‚
            â”‚         â”‚  Task Queue  â”‚          â”‚
            â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚
            â”‚                â”‚                â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Context Store  â”‚  (å…±äº«ä¸Šä¸‹æ–‡ã€çŠ¶æ€)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚  Skills   â”‚   â”‚  Memory   â”‚   â”‚  Knowledgeâ”‚
      â”‚  Engine   â”‚   â”‚  Service  â”‚   â”‚  Base    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Model Gateway  â”‚  (ç»Ÿä¸€LLMè°ƒç”¨)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
      â”‚ Fast Model  â”‚   â”‚  Strong Modelâ”‚   â”‚ TTS Engine â”‚
      â”‚  (Talker)  â”‚   â”‚  (Thinker)  â”‚   â”‚             â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                â”‚                â”‚
      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
      â”‚ LLM API #1  â”‚   â”‚ LLM API #2  â”‚
      â”‚ (OpenAI)    â”‚   â”‚ (Anthropic)  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Output Layer   â”‚  (Text, Audio, Video)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ ¸å¿ƒæ¨¡å—è®¾è®¡

#### 3.2.1 Talker Agentï¼ˆå¯¹è¯è€…ï¼‰

**æ ¸å¿ƒèŒè´£**ï¼š
1. **å¿«é€Ÿå“åº”**ï¼š
   - < 100msï¼šç›´æ¥å£è¯­å›åº”ï¼ˆ"å¥½çš„"ã€"æ˜ç™½äº†"ï¼‰
   - < 300msï¼šè°ƒç”¨è½»é‡çº§æ¨¡å‹å¿«é€Ÿå›ç­”
   - < 500msï¼šç®€å•ä»»åŠ¡å¤„ç†

2. **å®æ—¶æ’­æŠ¥**ï¼š
   - æ’­æŠ¥Thinkerçš„è¿›åº¦
   - è½¬æ¢Thinkerçš„"æ€è€ƒ"ä¸ºè‡ªç„¶å£è¯­
   - å¤„ç†Thinkerçš„è¾“å‡º

3. **ç”¨æˆ·äº¤äº’**ï¼š
   - è¯†åˆ«ç”¨æˆ·æ„å›¾ï¼ˆç®€å•/å¤æ‚ï¼‰
   - å†³å®šæ˜¯å¦éœ€è¦Thinker
   - ç®¡ç†å¯¹è¯èŠ‚å¥

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- **Model**ï¼šGPT-4.1-mini, Claude Haiku, æˆ–æ›´å°çš„æ¨¡å‹
- **Streaming**ï¼šæ”¯æŒæµå¼è¾“å‡º
- **Latency**ï¼šP50 < 200ms, P95 < 500ms
- **Context**ï¼šä¿ç•™æœ€è¿‘3-5è½®å¯¹è¯

#### 3.2.2 Thinker Agentï¼ˆæ€è€ƒè€…ï¼‰

**æ ¸å¿ƒèŒè´£**ï¼š
1. **æ·±åº¦æ¨ç†**ï¼š
   - å¤æ‚é€»è¾‘æ¨ç†
   - å¤šæ­¥ä»»åŠ¡è§„åˆ’
   - çŸ¥è¯†æ£€ç´¢å’Œæ•´åˆ

2. **ç”Ÿæˆæ€è€ƒ**ï¼š
   - ç”Ÿæˆ"æ€è€ƒ"token
   - è¾“å‡ºæ¨ç†è¿‡ç¨‹
   - æä¾›ä¸­é—´ç»“æœ

3. **è´¨é‡æ§åˆ¶**ï¼š
   - è‡ªæˆ‘åæ€ï¼ˆSelf-Reflectionï¼‰
   - å¤šè½®è¿­ä»£ä¼˜åŒ–
   - ç­”æ¡ˆéªŒè¯

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- **Model**ï¼šGPT-4.1, Claude Opus, æˆ–å…¶ä»–å¼ºå¤§æ¨¡å‹
- **Thinking**ï¼šChain of Thought, Tree of Thoughts
- **Context**ï¼šä¿ç•™å®Œæ•´ä»»åŠ¡é“¾å’Œä¸­é—´ç»“æœ
- **Latency**ï¼šå¯æ¥å—è¾ƒé•¿å»¶è¿Ÿï¼ˆ1-10sï¼‰

#### 3.2.3 Orchestratorï¼ˆåè°ƒå™¨ï¼‰

**æ ¸å¿ƒèŒè´£**ï¼š
1. **ä»»åŠ¡è°ƒåº¦**ï¼š
   - åˆ¤æ–­ä»»åŠ¡å¤æ‚åº¦
   - å†³å®šç”±Talkerè¿˜æ˜¯Thinkerå¤„ç†
   - åè°ƒä¸¤ä¸ªAgentçš„æ—¶åº

2. **Handoffç®¡ç†**ï¼š
   - ç®¡ç†Agentä¹‹é—´çš„äº¤æ¥
   - ç¡®ä¿ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
   - å¤„ç†å¼‚å¸¸å’Œå›æ»š

3. **çŠ¶æ€ç»´æŠ¤**ï¼š
   - ç»´æŠ¤å…¨å±€ä»»åŠ¡çŠ¶æ€
   - è¿½è¸ªAgentè¿›åº¦
   - å¤„ç†è¶…æ—¶å’Œé‡è¯•

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- **Language**ï¼šPython, Go, æˆ– Rust
- **State Machine**ï¼šä½¿ç”¨æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰
- **Message Queue**ï¼šRedis, RabbitMQ, æˆ– Kafka
- **Latency**ï¼š< 10ms for routing

#### 3.2.4 Context Storeï¼ˆä¸Šä¸‹æ–‡å­˜å‚¨ï¼‰

**åˆ†å±‚å­˜å‚¨**ï¼š

```
L1: Working Context (Memory)
â”œâ”€â”€ Current turn messages
â”œâ”€â”€ Agent states
â”œâ”€â”€ Temporal variables
â””â”€â”€ Latency: < 1ms

L2: Session Context (Redis)
â”œâ”€â”€ Conversation history (last 100 turns)
â”œâ”€â”€ Task states
â”œâ”€â”€ Intermediate results
â””â”€â”€ TTL: 24 hours

L3: Long-term Memory (PostgreSQL)
â”œâ”€â”€ User profiles
â”œâ”€â”€ Important events
â”œâ”€â”€ Learned patterns
â””â”€â”€ Persistent

L4: Knowledge Base (Vector DB)
â”œâ”€â”€ Domain knowledge
â”œâ”€â”€ RAG documents
â”œâ”€â”€ Embeddings
â””â”€â”€ Retrieval: semantic search
```

---

## 4. æ ¸å¿ƒååŒæœºåˆ¶è¯¦è§£

### 4.1 Handoffæ¨¡å¼è®¾è®¡

#### 4.1.1 å§”æ‰˜æ¨¡å¼ï¼ˆDelegationï¼‰

**åœºæ™¯**ï¼šTalkerè¯†åˆ«åˆ°å¤æ‚ä»»åŠ¡ï¼Œå§”æ‰˜ç»™Thinker

**æµç¨‹**ï¼š
```
1. ç”¨æˆ·è¾“å…¥
   â”‚
2. Talkeræ„å›¾åˆ†ç±»
   â”‚
   â”œâ”€â–º ç®€å•æ„å›¾ â†’ Talkerå¤„ç†
   â”‚
   â””â”€â–º å¤æ‚æ„å›¾ â†’ Handoff to Thinker
         â”‚
         3. Talkerç«‹å³å›å¤ï¼š"è¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘æ·±åº¦æ€è€ƒä¸€ä¸‹..."
         â”‚
         4. å¼‚æ­¥å¯åŠ¨Thinker
         â”‚
         5. Thinkerå¤„ç†ä»»åŠ¡
         â”‚
         6. Thinkerå®šæœŸæ¨é€è¿›åº¦
         â”‚
         7. Talkeræ’­æŠ¥è¿›åº¦
         â”‚
         8. Thinkerå®Œæˆ
         â”‚
         9. Talkerå±•ç¤ºæœ€ç»ˆç­”æ¡ˆ
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
async def delegation_handoff(user_input):
    # 1. Talkeræ„å›¾åˆ†ç±»
    intent = await talker.classify_intent(user_input)
    
    if intent["complexity"] == "simple":
        # Talkerç›´æ¥å¤„ç†
        response = await talker.process(user_input)
        return response
    else:
        # å¤æ‚æ„å›¾ï¼ŒHandoffåˆ°Thinker
        # 2. Talkerç«‹å³å›å¤
        yield "è¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘æ·±åº¦æ€è€ƒä¸€ä¸‹..."
        
        # 3. å¼‚æ­¥å¯åŠ¨Thinker
        task_id = str(uuid.uuid4())
        thinker_task = asyncio.create_task(
            thinker.process_with_feedback(
                task_id=task_id,
                task=user_input,
                feedback_callback=lambda msg: talker.broadcast(msg)
            )
        )
        
        # 4. Talkerå®šæœŸæ’­æŠ¥Thinkerè¿›åº¦
        while not thinker_task.done():
            progress = await thinker.get_progress(task_id)
            yield f"[è¿›åº¦] {progress}%"
            await asyncio.sleep(2)
        
        # 5. Thinkerå®Œæˆ
        result = await thinker_task
        yield f"[å®Œæˆ] {result['answer']}"
```

**ä¼˜ç‚¹**ï¼š
- ç”¨æˆ·ç«‹å³å¾—åˆ°åé¦ˆ
- Talkerå’ŒThinkerå¼‚æ­¥å·¥ä½œ
- è¿›åº¦é€æ˜

**ç¼ºç‚¹**ï¼š
- å¢åŠ ç³»ç»Ÿå¤æ‚åº¦
- éœ€è¦é¢å¤–çš„çŠ¶æ€ç®¡ç†

#### 4.1.2 å¹¶è¡Œæ¨¡å¼ï¼ˆParallelï¼‰

**åœºæ™¯**ï¼šTalkerå’ŒThinkeråŒæ—¶å·¥ä½œï¼ŒTalkeræä¾›å¿«é€Ÿåˆæ­¥ç­”æ¡ˆï¼ŒThinkeræä¾›è¯¦ç»†ç­”æ¡ˆ

**æµç¨‹**ï¼š
```
1. ç”¨æˆ·è¾“å…¥
   â”‚
2. å¹¶è¡Œå¯åŠ¨Talkerå’ŒThinker
   â”œâ”€â–º Talker: å¿«é€Ÿå“åº” (200ms)
   â””â”€â–º Thinker: æ·±åº¦æ€è€ƒ (5s)
   â”‚
3. Talkerå…ˆè¿”å›åˆæ­¥ç­”æ¡ˆ
   â”‚
4. Thinkerè¿”å›è¯¦ç»†ç­”æ¡ˆ
   â”‚
5. Talkerå†³å®šæ˜¯å¦å±•ç¤ºè¯¦ç»†ç­”æ¡ˆ
   â”œâ”€â–º å¦‚æœå·®å¼‚ä¸å¤§ â†’ ä¿æŒåŸç­”æ¡ˆ
   â””â”€â–º å¦‚æœå·®å¼‚å¤§ â†’ æ›´æ–°ç­”æ¡ˆ
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
async def parallel_handoff(user_input):
    # 1. å¹¶è¡Œå¯åŠ¨Talkerå’ŒThinker
    talker_task = asyncio.create_task(
        talker.quick_response(user_input)
    )
    thinker_task = asyncio.create_task(
        thinker.deep_think(user_input)
    )
    
    # 2. ç­‰å¾…Talkerå¿«é€Ÿå“åº”
    quick_answer = await talker_task
    yield quick_answer
    
    # 3. ç­‰å¾…Thinkerå®Œæˆï¼ˆä¸é˜»å¡ï¼‰
    detailed_answer = None
    while not thinker_task.done():
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if asyncio.get_event_loop().time() - start_time > 3.0:
            yield "[æç¤º] æˆ‘è¿˜åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨ç­‰..."
        await asyncio.sleep(1)
    
    detailed_answer = await thinker_task
    
    # 4. æ¯”è¾ƒä¸¤ä¸ªç­”æ¡ˆ
    if are_similar(quick_answer, detailed_answer):
        # å·®å¼‚ä¸å¤§ï¼Œä¸æ›´æ–°
        return quick_answer
    else:
        # å·®å¼‚å¤§ï¼Œæ›´æ–°ç­”æ¡ˆ
        yield f"\n\nã€æ›´è¯¦ç»†çš„ç­”æ¡ˆã€‘\n{detailed_answer}"
```

**ä¼˜ç‚¹**ï¼š
- æœ€å¤§åŒ–å“åº”é€Ÿåº¦
- æä¾›æ¸è¿›å¼ç­”æ¡ˆ
- ç”¨æˆ·ä½“éªŒå¥½

**ç¼ºç‚¹**ï¼š
- å¯èƒ½äº§ç”ŸçŸ›ç›¾
- éœ€è¦ç­”æ¡ˆæ¯”è¾ƒé€»è¾‘
- å¢åŠ APIè°ƒç”¨æˆæœ¬

#### 4.1.3 è¿­ä»£æ¨¡å¼ï¼ˆIterativeï¼‰

**åœºæ™¯**ï¼šTalkeræä¾›åˆæ­¥ç­”æ¡ˆï¼Œç”¨æˆ·åé¦ˆåï¼ŒThinkeræ”¹è¿›ç­”æ¡ˆ

**æµç¨‹**ï¼š
```
1. ç”¨æˆ·è¾“å…¥
   â”‚
2. Talkeræä¾›åˆæ­¥ç­”æ¡ˆ
   â”‚
3. ç”¨æˆ·åé¦ˆï¼ˆä¿®æ”¹/è¡¥å……/ç¡®è®¤ï¼‰
   â”‚
4. Thinkeræ ¹æ®åé¦ˆæ”¹è¿›ç­”æ¡ˆ
   â”‚
5. Talkerå±•ç¤ºæ”¹è¿›åçš„ç­”æ¡ˆ
   â”‚
6. é‡å¤3-5ï¼Œç›´åˆ°ç”¨æˆ·æ»¡æ„
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
async def iterative_handoff(user_input, max_rounds=3):
    context = {"original_input": user_input}
    
    for round_num in range(max_rounds):
        # 1. Talkeræä¾›ç­”æ¡ˆ
        if round_num == 0:
            answer = await talker.generate_answer(
                user_input,
                context=context
            )
        else:
            # åç»­è½®æ¬¡ï¼ŒåŸºäºåé¦ˆæ”¹è¿›
            answer = await thinker.improve_answer(
                previous_answers=context["answers"],
                feedback=context["feedback"],
                context=context
            )
        
        context["answers"] = context.get("answers", []) + [answer]
        yield f"[ç­”æ¡ˆ{round_num+1}]\n{answer}"
        
        # 2. ç­‰å¾…ç”¨æˆ·åé¦ˆ
        feedback = await wait_for_user_feedback()
        
        # 3. æ£€æŸ¥æ˜¯å¦æ»¡æ„
        if feedback == "satisfied":
            break
        elif feedback == "cancel":
            yield "å·²å–æ¶ˆ"
            return
        
        context["feedback"] = feedback
    
    # æœ€ç»ˆæ€»ç»“
    final_answer = await thinker.generate_final_summary(context)
    yield f"\n\nã€æœ€ç»ˆæ€»ç»“ã€‘\n{final_answer}"
```

**ä¼˜ç‚¹**ï¼š
- ç”¨æˆ·å‚ä¸åº¦é«˜
- ç­”æ¡ˆè´¨é‡æŒç»­æ”¹è¿›
- é€‚åˆå¤æ‚ä»»åŠ¡

**ç¼ºç‚¹**ï¼š
- å¢åŠ ç”¨æˆ·è´Ÿæ‹…
- ä¸é€‚åˆæ‰€æœ‰åœºæ™¯
- å¯èƒ½é™·å…¥è¿­ä»£å¾ªç¯

#### 4.1.4 åä½œæ¨¡å¼ï¼ˆCollaborationï¼‰

**åœºæ™¯**ï¼šTalkeræ”¶é›†ä¿¡æ¯ï¼ŒThinkeræ·±åº¦å¤„ç†ï¼ŒTalkerå®æ—¶æ’­æŠ¥

**æµç¨‹**ï¼š
```
1. ç”¨æˆ·è¾“å…¥
   â”‚
2. Talkerå¿«é€Ÿå“åº” + æ”¶é›†ä¿¡æ¯
   â”œâ”€â–º å›å¤ï¼š"å¥½çš„ï¼Œæˆ‘äº†è§£äº†ã€‚è®©æˆ‘æ·±åº¦åˆ†æä¸€ä¸‹..."
   â””â”€â–º æ”¶é›†ï¼šç”¨æˆ·åå¥½ã€ä¸Šä¸‹æ–‡ã€å†å²
   â”‚
3. Talker â†’ Thinker (Handoff)
   â”‚   â”œâ”€â–º ä¼ é€’ï¼šç”¨æˆ·è¾“å…¥ + æ”¶é›†çš„ä¿¡æ¯
   â”‚   â””â”€â–º å¼‚æ­¥å¯åŠ¨Thinker
   â”‚
4. Thinkerå¤„ç†ä»»åŠ¡ï¼ˆå¤šæ­¥éª¤ï¼‰
   â”œâ”€â–º æ­¥éª¤1... â†’ Talkeræ’­æŠ¥è¿›åº¦
   â”œâ”€â–º æ­¥éª¤2... â†’ Talkeræ’­æŠ¥è¿›åº¦
   â””â”€â–º æ­¥éª¤3... â†’ Talkeræ’­æŠ¥è¿›åº¦
   â”‚
5. Thinkerå®Œæˆ
   â”‚
6. Talkeræ€»ç»“å¹¶å±•ç¤ºç»“æœ
```

**ä»£ç ç¤ºä¾‹**ï¼š
```python
async def collaboration_handoff(user_input):
    # 1. Talkeræ”¶é›†ä¿¡æ¯
    collected_info = await talker.collect_info(user_input)
    
    # 2. Talkerå¿«é€Ÿå“åº”
    yield "å¥½çš„ï¼Œæˆ‘äº†è§£äº†ã€‚è®©æˆ‘æ·±åº¦åˆ†æä¸€ä¸‹..."
    
    # 3. Handoffåˆ°Thinker
    task_id = str(uuid.uuid4())
    thinker_task = asyncio.create_task(
        thinker.process_with_feedback(
            task_id=task_id,
            task=user_input,
            context=collected_info,
            feedback_callback=lambda msg: talker.broadcast_progress(msg)
        )
    )
    
    # 4. ç›‘æ§Thinkerè¿›åº¦
    while not thinker_task.done():
        await asyncio.sleep(1)
    
    # 5. Thinkerå®Œæˆ
    result = await thinker_task
    yield f"\n\nã€åˆ†æç»“æœã€‘\n{result['answer']}"
```

**ä¼˜ç‚¹**ï¼š
- æœ€å¤§åŒ–ååŒæ•ˆæœ
- ç”¨æˆ·å®æ—¶æ„ŸçŸ¥è¿›åº¦
- ä¸Šä¸‹æ–‡ä¸€è‡´æ€§é«˜

**ç¼ºç‚¹**ï¼š
- å®ç°å¤æ‚åº¦æœ€é«˜
- è°ƒè¯•å›°éš¾
- éœ€è¦ç²¾ç»†çš„æ—¶åºæ§åˆ¶

### 4.2 ä»»åŠ¡è°ƒåº¦ç­–ç•¥

#### 4.2.1 åŸºäºå¤æ‚åº¦çš„è°ƒåº¦

```python
class ComplexityBasedScheduler:
    def __init__(self):
        self.rules = {
            "simple": {
                "max_tokens": 500,
                "agent": "talker",
                "timeout": 1.0
            },
            "medium": {
                "max_tokens": 2000,
                "agent": "thinker",
                "timeout": 10.0
            },
            "complex": {
                "max_tokens": 8000,
                "agent": "thinker",
                "timeout": 60.0,
                "requires_planning": True
            }
        }
    
    async def schedule(self, task):
        # 1. è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦
        complexity = await self.evaluate_complexity(task)
        
        # 2. é€‰æ‹©ç­–ç•¥
        strategy = self.rules[complexity]
        
        # 3. æ‰§è¡Œä»»åŠ¡
        if strategy["agent"] == "talker":
            result = await talker.process(task, strategy)
        else:
            result = await thinker.process(task, strategy)
        
        return result
    
    async def evaluate_complexity(self, task):
        # åŸºäºå¤šä¸ªç»´åº¦è¯„ä¼°å¤æ‚åº¦
        features = {
            "length": len(task),
            "keywords": self.count_complex_keywords(task),
            "requires_tool": self.requires_tool(task),
            "multi_step": self.is_multi_step(task)
        }
        
        # åŠ æƒè¯„åˆ†
        score = (
            features["length"] * 0.2 +
            features["keywords"] * 0.3 +
            features["requires_tool"] * 0.3 +
            features["multi_step"] * 0.2
        )
        
        if score < 0.3:
            return "simple"
        elif score < 0.7:
            return "medium"
        else:
            return "complex"
```

#### 4.2.2 åŸºäºä¼˜å…ˆçº§çš„è°ƒåº¦

```python
class PriorityBasedScheduler:
    def __init__(self):
        self.queue = asyncio.PriorityQueue()
        self.active_tasks = {}
    
    async def schedule(self, task):
        # 1. è®¡ç®—ä¼˜å…ˆçº§
        priority = self.calculate_priority(task)
        
        # 2. åŠ å…¥é˜Ÿåˆ—
        await self.queue.put((priority, task))
        
        # 3. å¯åŠ¨è°ƒåº¦å™¨
        asyncio.create_task(self.scheduler_loop())
    
    async def scheduler_loop(self):
        while True:
            # è·å–æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
            priority, task = await self.queue.get()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨Agent
            if self.has_available_agent(task):
                # æ‰§è¡Œä»»åŠ¡
                asyncio.create_task(self.execute_task(task))
            else:
                # é‡æ–°å…¥é˜Ÿï¼Œç­‰å¾…Agentå¯ç”¨
                await self.queue.put((priority, task))
            
            await asyncio.sleep(0.1)
    
    def calculate_priority(self, task):
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ä¼˜å…ˆçº§
        factors = {
            "user_importance": task.get("importance", "normal"),
            "deadline": task.get("deadline", None),
            "waiting_time": time.time() - task.get("created_at", time.time()),
            "complexity": task.get("complexity", "medium")
        }
        
        priority_score = 0
        if factors["user_importance"] == "urgent":
            priority_score -= 10
        if factors["deadline"]:
            time_remaining = factors["deadline"] - time.time()
            if time_remaining < 300:  # 5åˆ†é’Ÿå†…
                priority_score -= 5
        priority_score += factors["waiting_time"] / 60.0
        if factors["complexity"] == "simple":
            priority_score += 5
        
        return priority_score
```

### 4.3 ä¸Šä¸‹æ–‡åŒæ­¥æœºåˆ¶

#### 4.3.1 å…±äº«å†…å­˜æ¨¡å¼

```python
class SharedMemoryContext:
    def __init__(self):
        self.context = {
            "messages": [],
            "agent_states": {},
            "shared_data": {},
            "version": 0
        }
        self.lock = asyncio.Lock()
    
    async def update_context(self, key, value):
        async with self.lock:
            self.context[key] = value
            self.context["version"] += 1
    
    async def get_context(self, key, version=None):
        async with self.lock:
            if version is None or version == self.context["version"]:
                return self.context.get(key)
            return None
    
    async def get_full_context(self, min_version=None):
        async with self.lock:
            if min_version is None or min_version >= self.context["version"]:
                return self.context.copy()
            return None
```

#### 4.3.2 äº‹ä»¶æº¯æºæ¨¡å¼

```python
class EventSourcedContext:
    def __init__(self, event_store):
        self.event_store = event_store
    
    async def apply_event(self, event):
        # 1. å­˜å‚¨äº‹ä»¶
        await self.event_store.append(event)
        
        # 2. é‡æ”¾äº‹ä»¶è·å–å½“å‰çŠ¶æ€
        current_state = await self.replay_events()
        return current_state
    
    async def replay_events(self):
        events = await self.event_store.get_all()
        state = {}
        for event in events:
            state = apply_event_to_state(state, event)
        return state
    
    async def get_context_at_version(self, version):
        # é‡æ”¾äº‹ä»¶åˆ°æŒ‡å®šç‰ˆæœ¬
        events = await self.event_store.get_until_version(version)
        state = {}
        for event in events:
            state = apply_event_to_state(state, event)
        return state
```

---

## 5. ä¸Šä¸‹æ–‡å…±äº«æ¶æ„

### 5.1 åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†

#### 5.1.1 L1: Working Contextï¼ˆå·¥ä½œä¸Šä¸‹æ–‡ï¼‰

**å­˜å‚¨**ï¼šå†…å­˜
**TTL**ï¼šä¼šè¯æœŸé—´
**å†…å®¹**ï¼š
```python
class WorkingContext:
    def __init__(self):
        self.messages = []  # å½“å‰è½®æ¬¡æ¶ˆæ¯
        self.agent_states = {}  # AgentçŠ¶æ€
        self.temp_vars = {}  # ä¸´æ—¶å˜é‡
        self.metadata = {
            "start_time": time.time(),
            "turn_id": str(uuid.uuid4())
        }
    
    def add_message(self, role, content):
        self.messages.append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })
    
    def set_agent_state(self, agent_name, state):
        self.agent_states[agent_name] = {
            "state": state,
            "timestamp": time.time()
        }
    
    def get_recent_context(self, n=10):
        return {
            "messages": self.messages[-n:],
            "agent_states": self.agent_states,
            "metadata": self.metadata
        }
```

#### 5.1.2 L2: Session Contextï¼ˆä¼šè¯ä¸Šä¸‹æ–‡ï¼‰

**å­˜å‚¨**ï¼šRedis
**TTL**ï¼š24å°æ—¶
**å†…å®¹**ï¼š
```python
class SessionContext:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def add_message(self, session_id, message):
        key = f"session:{session_id}:messages"
        await self.redis.lpush(key, json.dumps(message))
        await self.redis.ltrim(key, 0, 99)  # åªä¿ç•™æœ€è¿‘100æ¡
    
    async def get_messages(self, session_id, limit=50):
        key = f"session:{session_id}:messages"
        messages = await self.redis.lrange(key, -limit, -1)
        return [json.loads(msg) for msg in messages[::-1]]
    
    async def set_task_state(self, session_id, task_id, state):
        key = f"session:{session_id}:task:{task_id}"
        await self.redis.hset(key, mapping=json.dumps(state))
        await self.redis.expire(key, 3600)  # 1å°æ—¶è¿‡æœŸ
    
    async def get_task_state(self, session_id, task_id):
        key = f"session:{session_id}:task:{task_id}"
        return await self.redis.hgetall(key)
```

#### 5.1.3 L3: Long-term Memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰

**å­˜å‚¨**ï¼šPostgreSQL
**TTL**ï¼šæ°¸ä¹…
**å†…å®¹**ï¼š
```python
class LongTermMemory:
    def __init__(self, db_connection):
        self.db = db_connection
    
    async def save_event(self, user_id, event_type, event_data):
        query = """
        INSERT INTO user_events (user_id, event_type, event_data, timestamp)
        VALUES ($1, $2, $3, $4)
        """
        await self.db.execute(query, user_id, event_type, json.dumps(event_data), time.time())
    
    async def get_user_profile(self, user_id):
        query = """
        SELECT * FROM user_profiles
        WHERE user_id = $1
        """
        result = await self.db.fetchrow(query, user_id)
        return dict(result) if result else None
    
    async def update_user_profile(self, user_id, updates):
        query = """
        INSERT INTO user_profiles (user_id, profile_data, updated_at)
        VALUES ($1, $2, $3)
        ON CONFLICT (user_id) DO UPDATE SET
            profile_data = user_profiles.profile_data || $2,
            updated_at = $3
        """
        await self.db.execute(query, user_id, json.dumps(updates), time.time())
```

#### 5.1.4 L4: Knowledge Baseï¼ˆçŸ¥è¯†åº“ï¼‰

**å­˜å‚¨**ï¼šå‘é‡æ•°æ®åº“ï¼ˆå¦‚Milvusã€Pineconeã€Qdrantï¼‰
**TTL**ï¼šæ°¸ä¹…
**å†…å®¹**ï¼š
```python
class KnowledgeBase:
    def __init__(self, vector_db_client, embedding_model):
        self.vector_db = vector_db_client
        self.embedding_model = embedding_model
    
    async def add_knowledge(self, content, metadata):
        # 1. ç”Ÿæˆembedding
        embedding = await self.embedding_model.encode(content)
        
        # 2. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        await self.vector_db.insert({
            "content": content,
            "embedding": embedding,
            "metadata": metadata
        })
    
    async def search(self, query, top_k=5, filters=None):
        # 1. ç”ŸæˆæŸ¥è¯¢embedding
        query_embedding = await self.embedding_model.encode(query)
        
        # 2. å‘é‡æ£€ç´¢
        results = await self.vector_db.search(
            query_embedding,
            top_k=top_k,
            filters=filters
        )
        return results
    
    async def retrieve_with_context(self, query, top_k=5):
        results = await self.search(query, top_k)
        context = "\n\n".join([
            f"ã€çŸ¥è¯†{i+1}ã€‘\n{r['content']}"
            for i, r in enumerate(results)
        ])
        return context, results
```

### 5.2 ä¸Šä¸‹æ–‡å‹ç¼©ä¸æ‘˜è¦

#### 5.2.1 å¯¹è¯æ‘˜è¦

```python
class ConversationSummarizer:
    def __init__(self, llm_client):
        self.llm = llm_client
        self.summary_threshold = 10  # æ¯10æ¡æ¶ˆæ¯æ€»ç»“ä¸€æ¬¡
    
    async def summarize_recent_messages(self, messages, max_tokens=500):
        if len(messages) < self.summary_threshold:
            return None
        
        prompt = f"""
        è¯·å°†ä»¥ä¸‹å¯¹è¯æ‘˜è¦æˆä¸€æ®µç®€çŸ­æ–‡å­—ï¼ˆä¸è¶…è¿‡{max_tokens} tokensï¼‰ï¼š

        {format_messages(messages)}

        æ‘˜è¦è¦æ±‚ï¼š
        1. ä¿ç•™å…³é”®ä¿¡æ¯å’Œå†³ç­–
        2. çœç•¥æ— å…³çš„é—²èŠ
        3. æŒ‰æ—¶é—´é¡ºåºç»„ç»‡
        4. çªå‡ºé‡è¦äº‹ä»¶å’Œç»“è®º
        """
        
        summary = await self.llm.generate(prompt, max_tokens=max_tokens)
        return summary
    
    async def incremental_summarize(self, session_id, messages):
        # 1. è·å–ä¹‹å‰çš„æ‘˜è¦
        previous_summary = await self.get_previous_summary(session_id)
        
        # 2. è·å–æ–°æ¶ˆæ¯
        new_messages = messages[self.last_summary_count:]
        
        # 3. ç”Ÿæˆæ–°æ‘˜è¦
        if previous_summary:
            prompt = f"""
            ä¹‹å‰çš„æ‘˜è¦ï¼š{previous_summary}

            æ–°å¢çš„å¯¹è¯ï¼š
            {format_messages(new_messages)}

            è¯·æ›´æ–°æ‘˜è¦ï¼Œæ•´åˆæ–°ä¿¡æ¯ã€‚
            """
            summary = await self.llm.generate(prompt, max_tokens=500)
        else:
            summary = await self.summarize_recent_messages(messages)
        
        # 4. ä¿å­˜æ–°æ‘˜è¦
        await self.save_summary(session_id, summary)
        
        # 5. æ¸…ç†æ—§æ¶ˆæ¯
        await self.clean_old_messages(session_id, len(new_messages))
        
        return summary
```

#### 5.2.2 æ¸è¿›å¼æ‘˜è¦

```python
class ProgressiveSummarizer:
    def __init__(self):
        self.summaries = []
        self.message_indices = []
    
    async def add_messages(self, messages):
        self.messages.extend(messages)
        current_index = len(self.summaries)
        
        # æ¯10æ¡æ¶ˆæ¯ç”Ÿæˆä¸€æ¬¡æ‘˜è¦
        if len(self.messages) - self.message_indices[-1] if self.message_indices else 0 >= 10:
            summary = await self._summarize_chunk(
                self.messages[self.message_indices[-1]:]
            )
            self.summaries.append(summary)
            self.message_indices.append(len(self.messages))
    
    async def get_full_context(self):
        # 1. ä¹‹å‰çš„æ‘˜è¦
        context = "\n\n".join([
            f"[æ‘˜è¦{i+1}] {summary}"
            for i, summary in enumerate(self.summaries)
        ])
        
        # 2. æœ€è¿‘çš„åŸå§‹æ¶ˆæ¯ï¼ˆä¸æ‘˜è¦ï¼‰
        if self.message_indices:
            recent_messages = self.messages[self.message_indices[-1]:]
        else:
            recent_messages = self.messages[-5:]
        context += "\n\n" + format_messages(recent_messages)
        
        return context
    
    async def _summarize_chunk(self, messages):
        prompt = f"""
        è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯ï¼š
        {format_messages(messages)}
        
        åªè¾“å‡ºæ‘˜è¦ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
        """
        summary = await llm_generate(prompt, max_tokens=300)
        return summary
```

### 5.3 ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ä¿è¯

#### 5.3.1 ä¹è§‚é”

```python
class ContextWithOptimisticLock:
    def __init__(self, context_store):
        self.store = context_store
        self.locks = defaultdict(asyncio.Lock)
    
    async def update(self, key, update_fn):
        lock_key = f"context:{key}"
        async with self.locks[lock_key]:
            # 1. è·å–å½“å‰å€¼
            current = await self.store.get(key)
            
            # 2. åº”ç”¨æ›´æ–°
            updated = update_fn(current)
            updated["version"] = current.get("version", 0) + 1
            updated["updated_at"] = time.time()
            
            # 3. å°è¯•å†™å…¥ï¼ˆå¸¦ç‰ˆæœ¬æ£€æŸ¥ï¼‰
            success = await self.store.compare_and_set(
                key,
                expected_version=current.get("version", 0),
                new_value=updated
            )
            
            if not success:
                # å†²çªï¼Œé‡è¯•
                await asyncio.sleep(0.1 * random.randint(1, 3))
                return await self.update(key, update_fn)
            
            return updated
    
    async def get(self, key, version=None):
        value = await self.store.get(key)
        if version is None or value.get("version", 0) == version:
            return value
        return None
```

#### 5.3.2 æœ€ç»ˆä¸€è‡´æ€§åè®®

```python
class EventSourcedContext:
    def __init__(self, event_store):
        self.store = event_store
    
    async def apply_event(self, event):
        # 1. å­˜å‚¨äº‹ä»¶ï¼ˆå¸¦å”¯ä¸€IDï¼‰
        event_id = str(uuid.uuid4())
        event["event_id"] = event_id
        event["timestamp"] = time.time()
        
        await self.store.append(event)
        
        # 2. é‡æ”¾æ‰€æœ‰äº‹ä»¶è·å–å½“å‰çŠ¶æ€
        current_state = await self.replay_events()
        return current_state
    
    async def resolve_conflict(self, base_version, conflict_events):
        # ä½¿ç”¨æœ€åå†™å…¥ä¸ºå‡†
        final_state = {}
        for event in conflict_events:
            final_state = apply_event_to_state(final_state, event)
        
        # åˆ›å»ºè§£å†³äº‹ä»¶
        resolution_event = {
            "type": "conflict_resolution",
            "base_version": base_version,
            "conflicting_events": [e["event_id"] for e in conflict_events],
            "final_state": final_state,
            "resolved_at": time.time()
        }
        
        await self.store.append(resolution_event)
        return final_state
```

---

## 6. Promptå·¥ç¨‹ç­–ç•¥

### 6.1 Talker Promptè®¾è®¡

#### 6.1.1 åŸºç¡€Prompt

```markdown
# è§’è‰²å®šä¹‰
ä½ å«Talkerï¼Œæ˜¯ä¸€ä¸ªå‹å¥½ã€é«˜æ•ˆçš„å¯¹è¯åŠ©æ‰‹ã€‚

## ä½ çš„ç‰¹ç‚¹
- å“åº”å¿«é€Ÿï¼ˆé¿å…é•¿ç¯‡å¤§è®ºï¼‰
- è¯­æ°”è½»æ¾ï¼ˆå¶å°”å¹½é»˜ï¼‰
- ç›´æ¥å›ç­”ï¼ˆä¸ç»•å¼¯å­ï¼‰
- ä¸ç¡®å®šæ—¶è¯šå®å‘ŠçŸ¥

## ä½ çš„ä»»åŠ¡
æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œç›´æ¥ç»™å‡ºç®€æ´ç­”æ¡ˆã€‚

## è¾“å‡ºè§„åˆ™
1. å¦‚æœèƒ½ç›´æ¥å›ç­”ï¼šç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œæ§åˆ¶åœ¨100å­—ä»¥å†…
2. å¦‚æœéœ€è¦æ·±åº¦æ€è€ƒï¼šè¾“å‡º "è¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘æ€è€ƒä¸€ä¸‹..."
3. å¦‚æœä¸ç¡®å®šï¼šè¯´"æˆ‘ä¸å¤ªç¡®å®šï¼Œè®©æˆ‘æŸ¥æŸ¥"
4. å¦‚æœéœ€è¦æ›´å¤šä¿¡æ¯ï¼šä¸»åŠ¨è¯¢é—®ç”¨æˆ·

## ç¤ºä¾‹
ç”¨æˆ·ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ
ä½ ï¼šä»Šå¤©åŒ—äº¬æ™´å¤©ï¼Œæ°”æ¸©18-26åº¦ï¼Œå¾ˆé€‚åˆå‡ºé—¨ï¼

ç”¨æˆ·ï¼šé‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆï¼Ÿ
ä½ ï¼šè¿™ä¸ªé—®é¢˜æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘æ·±åº¦æ€è€ƒä¸€ä¸‹...

ç”¨æˆ·ï¼š1+1ç­‰äºå‡ ï¼Ÿ
ä½ ï¼š1+1ç­‰äº2ï¼Œè¿™ä¸ªæˆ‘çŸ¥é“ï¼

## æ³¨æ„äº‹é¡¹
- ä¸è¦ç¼–é€ äº‹å®
- ä¸è¦ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼ˆé™¤éç”¨æˆ·ä½¿ç”¨ï¼‰
- ä¿æŒå£è¯­åŒ–
```

#### 6.1.2 æ„å›¾åˆ†ç±»Prompt

```markdown
# ä»»åŠ¡ï¼šå¿«é€Ÿæ„å›¾åˆ†ç±»

è¯·åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·è¯·æ±‚å±äºå“ªç±»æ„å›¾ï¼š

## æ„å›¾ç±»å‹
1. **é—®å€™/å¯’æš„** - å¦‚"ä½ å¥½"ã€"åœ¨å—"
2. **ç®€å•é—®ç­”** - ç›´æ¥å¯ä»¥å›ç­”çš„é—®é¢˜
3. **å¤æ‚æ¨ç†** - éœ€è¦å¤šæ­¥æ€è€ƒ
4. **éœ€è¦å·¥å…·** - éœ€è¦è°ƒç”¨å¤–éƒ¨API
5. **éœ€è¦è§„åˆ’** - éœ€è¦æ‹†è§£ä»»åŠ¡
6. **é—²èŠ** - éæ­£å¼å¯¹è¯

## ç”¨æˆ·è¾“å…¥
{user_input}

## ä¸Šä¸‹æ–‡
{context}

## è¾“å‡ºæ ¼å¼
åªè¿”å›æ„å›¾ç¼–å·ï¼ˆ1-6ï¼‰ï¼Œä¸è¦è§£é‡Šã€‚

ä¾‹å¦‚ï¼š
ç”¨æˆ·ï¼šä½ å¥½
è¾“å‡ºï¼š1

ç”¨æˆ·ï¼šé‡å­åŠ›å­¦æ˜¯ä»€ä¹ˆï¼Ÿ
è¾“å‡ºï¼š3

## çº¦æŸ
- å¦‚æœæœ‰å¤šç§å¯èƒ½ï¼Œé€‰æ‹©æœ€å¯èƒ½çš„ä¸€ä¸ª
- å¦‚æœæ— æ³•ç¡®å®šï¼Œè¿”å›6ï¼ˆé—²èŠï¼‰
```

#### 6.1.3 è¿›åº¦æ’­æŠ¥Prompt

```markdown
# ä»»åŠ¡ï¼šæ’­æŠ¥Thinkerçš„å¤„ç†è¿›åº¦

## ä½ çš„èŒè´£
ç”¨å‹å¥½ã€ç®€æ´çš„æ–¹å¼å‘ŠçŸ¥ç”¨æˆ·Thinkeræ­£åœ¨åšä»€ä¹ˆã€‚

## è¾“å…¥ä¿¡æ¯
- åŸå§‹é—®é¢˜ï¼š{original_question}
- Thinkerå½“å‰æ­¥éª¤ï¼š{current_step}
- æ€»æ­¥éª¤æ•°ï¼š{total_steps}
- é¢„è®¡å‰©ä½™æ—¶é—´ï¼š{estimated_time}ç§’

## è¾“å‡ºè¦æ±‚
1. ä¿æŒä¹è§‚ã€é¼“åŠ±çš„è¯­æ°”
2. é¿å…æŠ€æœ¯ç»†èŠ‚
3. æ¯æ¡æ’­æŠ¥ä¸è¶…è¿‡30å­—
4. å¯ä»¥ç”¨emojiå¢åŠ è¶£å‘³æ€§

## è¾“å‡ºæ¨¡æ¿
æ ¹æ®ä¸åŒé˜¶æ®µé€‰æ‹©ï¼š

- åˆå§‹é˜¶æ®µï¼š"è®©æˆ‘å…ˆåˆ†æä¸€ä¸‹ä½ çš„é—®é¢˜..."
- æ‰§è¡Œä¸­ï¼š"æ­£åœ¨å¤„ç†ä¸­... ({current}/{total})"
- å³å°†å®Œæˆï¼š"é©¬ä¸Šå°±å¥½ï¼æœ€åä¸€æ­¥äº† ğŸ‰"
- é‡åˆ°é—®é¢˜ï¼š"æœ‰ç‚¹å¤æ‚ï¼Œè®©æˆ‘å†æƒ³æƒ³ ğŸ¤”"

## ç¤ºä¾‹
è¾“å…¥ï¼š{"current_step": "åˆ†æéœ€æ±‚", "total_steps": 5, "estimated_time": 20}
è¾“å‡ºï¼š"æ­£åœ¨åˆ†æä½ çš„éœ€æ±‚... (1/5)"

è¾“å…¥ï¼š{"current_step": "ç”Ÿæˆç­”æ¡ˆ", "total_steps": 5, "estimated_time": 2}
è¾“å‡ºï¼š"é©¬ä¸Šå°±å¥½ï¼æœ€åä¸€æ­¥äº† ğŸ‰"
```

#### 6.1.4 ä¸»åŠ¨è¯é¢˜æ‰©å±•Prompt

```markdown
# ä»»åŠ¡ï¼šåŸºäºä¸Šä¸‹æ–‡ä¸»åŠ¨å‘èµ·è¯é¢˜

## ä½ çš„èŒè´£
åœ¨å¯¹è¯é—´éš™ï¼Œä¸»åŠ¨å‘èµ·ç›¸å…³ä½†ä¸çªå…€çš„è¯é¢˜ã€‚

## è¾“å…¥
- å¯¹è¯å†å²ï¼š{conversation_history}
- ç”¨æˆ·å…´è¶£ï¼š{user_interests}
- å½“å‰ä¸Šä¸‹æ–‡ï¼š{current_context}

## å†³ç­–æµç¨‹
1. åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸»åŠ¨å‘èµ·è¯é¢˜
   - å¦‚æœç”¨æˆ·åˆšé—®å®Œå¤æ‚é—®é¢˜ï¼Œä¸è¦ç«‹å³å‘èµ·ï¼ˆè®©ç”¨æˆ·æ¶ˆåŒ–ï¼‰
   - å¦‚æœå¯¹è¯æœ‰å†·åœºé£é™©ï¼ˆè¶…è¿‡30ç§’æ— è¾“å…¥ï¼‰ï¼Œè€ƒè™‘å‘èµ·
   - å¦‚æœå‘ç°äº†ä¸ç”¨æˆ·å…´è¶£ç›¸å…³çš„æœ‰è¶£å†…å®¹ï¼Œå¯ä»¥å‘èµ·

2. é€‰æ‹©åˆé€‚çš„è¯é¢˜
   - ä¸å½“å‰ä¸Šä¸‹æ–‡ç›¸å…³
   - ç¬¦åˆç”¨æˆ·å…´è¶£
   - è½»æ¾ã€æœ‰è¶£ï¼ˆéä¸¥è‚ƒè¯é¢˜ï¼‰

3. æ„å»ºè‡ªç„¶çš„è¿‡æ¸¡å¥
   - é¿å…çªç„¶ï¼š"å¯¹äº†ï¼Œæˆ‘æƒ³åˆ°..."
   - ä½¿ç”¨æ‰¿æ¥ï¼š"è¯´åˆ°è¿™ä¸ªï¼Œå…¶å®..."

## è¾“å‡º
- å¦‚æœä¸éœ€è¦å‘èµ·ï¼šè¾“å‡º "NO_ACTION"
- å¦‚æœéœ€è¦å‘èµ·ï¼šè¾“å‡ºè¿‡æ¸¡å¥ + è¯é¢˜å†…å®¹

## ç¤ºä¾‹è¾“å‡º
- ä¸éœ€è¦å‘èµ·ï¼šNO_ACTION
- éœ€è¦å‘èµ·ï¼š"è¯´åˆ°è¿™ä¸ªï¼Œæˆ‘æœ€è¿‘çœ‹åˆ°ä¸€ç¯‡å¾ˆæœ‰æ„æ€çš„æ–‡ç« ï¼Œè®²çš„æ˜¯..."
- éœ€è¦å‘èµ·ï¼š"é¡ºä¾¿é—®ä¸€ä¸‹ï¼Œä½ å¹³æ—¶å…³æ³¨è¿™ä¸ªé¢†åŸŸå—ï¼Ÿ"
```

### 6.2 Thinker Promptè®¾è®¡

#### 6.2.1 ä»»åŠ¡è§„åˆ’Prompt

```markdown
# è§’è‰²ï¼šä»»åŠ¡è§„åˆ’ä¸“å®¶

## ä½ çš„èŒè´£
å°†å¤æ‚çš„ç”¨æˆ·è¯·æ±‚æ‹†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ã€‚

## è¾“å…¥
- ç”¨æˆ·è¯·æ±‚ï¼š{user_request}
- å¯ç”¨Skillsï¼š{available_skills}
- ä¸Šä¸‹æ–‡ï¼š{context}
- æ—¶é—´é¢„ç®—ï¼š{time_budget}ç§’

## è¾“å‡ºè¦æ±‚

### 1. æ„å›¾ç†è§£
é¦–å…ˆï¼Œç”¨ä¸€å¥è¯æ¦‚æ‹¬ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾ã€‚

### 2. çº¦æŸè¯†åˆ«
è¯†åˆ«ç”¨æˆ·æåˆ°çš„çº¦æŸï¼ˆæ—¶é—´ã€æ ¼å¼ã€åå¥½ç­‰ï¼‰ã€‚

### 3. æ­¥éª¤æ‹†è§£
å°†ä»»åŠ¡æ‹†è§£ä¸º3-7ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…æ‹¬ï¼š
- **æ­¥éª¤åç§°**ï¼šç®€æ´æè¿°
- **è¯¦ç»†æè¿°**ï¼šå…·ä½“è¦åšä»€ä¹ˆ
- **æ‰€éœ€Skills**ï¼šéœ€è¦è°ƒç”¨å“ªäº›Skills
- **é¢„æœŸè¾“å‡º**ï¼šæœŸæœ›çš„ç»“æœæ ¼å¼
- **ä¾èµ–å…³ç³»**ï¼šä¾èµ–å“ªäº›å‰ç½®æ­¥éª¤

### 4. é£é™©è¯„ä¼°
è¯†åˆ«å¯èƒ½çš„é£é™©ç‚¹ï¼š
- å“ªäº›æ­¥éª¤å¯èƒ½å¤±è´¥ï¼Ÿ
- å¦‚æœå¤±è´¥ï¼Œå¦‚ä½•é™çº§ï¼Ÿ
- æ€»æ—¶é—´æ˜¯å¦è¶³å¤Ÿï¼Ÿ

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
  "intent": "ç”¨æˆ·çš„æ ¸å¿ƒæ„å›¾",
  "constraints": ["çº¦æŸ1", "çº¦æŸ2"],
  "steps": [
    {
      "name": "æ­¥éª¤åç§°",
      "description": "è¯¦ç»†æè¿°",
      "skills": ["skill1", "skill2"],
      "expected_output": "é¢„æœŸè¾“å‡ºæ ¼å¼",
      "dependencies": ["å‰ç½®æ­¥éª¤å"],
      "fallback": "å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ"
    }
  ],
  "risks": [
    {"step": "æ­¥éª¤å", "risk": "é£é™©æè¿°", "mitigation": "ç¼“è§£æªæ–½"}
  ],
  "estimated_time": 120
}}
```

## çº¦æŸ
- æ­¥éª¤åº”è¯¥æ˜¯ç‹¬ç«‹çš„ã€å¯æ‰§è¡Œçš„
- æ­¥éª¤ä¹‹é—´æœ‰æ¸…æ™°çš„ä¾èµ–å…³ç³»
- å¦‚æœæŸä¸ªæ­¥éª¤æ— æ³•ç¡®å®šï¼Œæ ‡è®°ä¸º "needs_clarification"
```

#### 6.2.2 æ­¥éª¤æ‰§è¡ŒPrompt

```markdown
# è§’è‰²ï¼šæ­¥éª¤æ‰§è¡Œè€…

## ä½ çš„èŒè´£
æ‰§è¡Œå•ä¸ªä»»åŠ¡æ­¥éª¤ï¼Œå¹¶è¾“å‡ºç»“æ„åŒ–ç»“æœã€‚

## è¾“å…¥
- æ­¥éª¤å®šä¹‰ï¼š{step_definition}
- ä¸Šä¸‹æ–‡ï¼š{context}
- ä¹‹å‰æ­¥éª¤çš„ç»“æœï¼š{previous_results}

## æ‰§è¡Œæµç¨‹
1. **åˆ†æéœ€æ±‚**ï¼šç†è§£æ­¥éª¤è¦åšä»€ä¹ˆ
2. **å‡†å¤‡å‚æ•°**ï¼šä»ä¸Šä¸‹æ–‡å’Œä¹‹å‰ç»“æœä¸­æå–å‚æ•°
3. **è°ƒç”¨Skills**ï¼šæŒ‰é¡ºåºè°ƒç”¨æ‰€éœ€çš„Skills
4. **å¤„ç†å¼‚å¸¸**ï¼šå¦‚æœSkillå¤±è´¥ï¼Œå°è¯•æ¢å¤æˆ–é™çº§
5. **æ•´åˆç»“æœ**ï¼šå°†å¤šä¸ªSkillçš„ç»“æœæ•´åˆ
6. **è¾“å‡ºç»“æœ**ï¼šæŒ‰è¦æ±‚çš„æ ¼å¼è¾“å‡º

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
  "status": "success" | "failed" | "partial_success",
  "result": "æ­¥éª¤çš„æœ€ç»ˆç»“æœ",
  "intermediate_results": [
    {"skill": "skill1", "output": "è¾“å‡º1"},
    {"skill": "skill2", "output": "è¾“å‡º2"}
  ],
  "next_action": "ä¸‹ä¸€æ­¥çš„å»ºè®®",
  "metadata": {
    "skills_called": ["skill1", "skill2"],
    "latency_ms": 1234,
    "tokens_used": 567,
    "errors": []
  }
}}
```

## å¼‚å¸¸å¤„ç†
å¦‚æœSkillè°ƒç”¨å¤±è´¥ï¼š
- è®°å½•é”™è¯¯ä¿¡æ¯
- å°è¯•æœ€å¤š3æ¬¡
- å¦‚æœä»ç„¶å¤±è´¥ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
- åœ¨errorsä¸­è®°å½•è¯¦ç»†ä¿¡æ¯

## çº¦æŸ
- ä¿æŒè¾“å‡ºç®€æ´ï¼Œåªä¿ç•™å¿…è¦ä¿¡æ¯
- å¦‚æœå¤±è´¥ï¼Œè¯´æ˜åŸå› å’Œå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ
```

#### 6.2.3 è‡ªæˆ‘åæ€Prompt

```markdown
# è§’è‰²ï¼šè´¨é‡æ£€æŸ¥å‘˜

## ä½ çš„èŒè´£
æ£€æŸ¥Thinkerçš„è¾“å‡ºè´¨é‡ï¼Œæå‡ºæ”¹è¿›å»ºè®®ã€‚

## è¾“å…¥
- åŸå§‹é—®é¢˜ï¼š{original_question}
- Thinkerçš„å›ç­”ï¼š{thinker_answer}
- æ‰§è¡Œè¿‡ç¨‹ï¼š{execution_process}
- ä¸Šä¸‹æ–‡ï¼š{context}

## æ£€æŸ¥ç»´åº¦

### 1. å®Œæ•´æ€§ï¼ˆ20åˆ†ï¼‰
- æ˜¯å¦å®Œæ•´å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
- æ˜¯å¦é—æ¼äº†é‡è¦ä¿¡æ¯ï¼Ÿ

### 2. å‡†ç¡®æ€§ï¼ˆ30åˆ†ï¼‰
- é€»è¾‘æ˜¯å¦è‡ªæ´½ï¼Ÿ
- äº‹å®æ˜¯å¦å‡†ç¡®ï¼Ÿ
- æ˜¯å¦æœ‰çŸ›ç›¾çš„åœ°æ–¹ï¼Ÿ

### 3. ç›¸å…³æ€§ï¼ˆ20åˆ†ï¼‰
- æ˜¯å¦é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜ï¼Ÿ
- æ˜¯å¦åŒ…å«äº†ä¸å¿…è¦çš„ä¿¡æ¯ï¼Ÿ

### 4. æ¸…æ™°æ€§ï¼ˆ15åˆ†ï¼‰
- è¯­è¨€æ˜¯å¦æ¸…æ™°æ˜“æ‡‚ï¼Ÿ
- ç»“æ„æ˜¯å¦åˆç†ï¼Ÿ
- æ˜¯å¦éœ€è¦æ›´å¤šè§£é‡Šï¼Ÿ

### 5. å®ç”¨æ€§ï¼ˆ15åˆ†ï¼‰
- æ˜¯å¦å¯¹ç”¨æˆ·æœ‰å¸®åŠ©ï¼Ÿ
- æ˜¯å¦æä¾›äº†å¯æ“ä½œçš„å»ºè®®ï¼Ÿ

## è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰
```json
{{
  "overall_score": 85,
  "dimensions": {
    "completeness": 90,
    "accuracy": 85,
    "relevance": 90,
    "clarity": 80,
    "usefulness": 80
  },
  "issues": ["é—®é¢˜1", "é—®é¢˜2"],
  "suggestions": ["å»ºè®®1", "å»ºè®®2"],
  "needs_revision": true,
  "reasoning": "è™½ç„¶ç­”æ¡ˆåŸºæœ¬æ­£ç¡®ï¼Œä½†ç¼ºå°‘ç”¨æˆ·å…³å¿ƒçš„ç»†èŠ‚ï¼Œå»ºè®®è¡¥å……"
}
```

## å†³ç­–è§„åˆ™
- å¦‚æœoverall_score >= 90ï¼šæ— éœ€ä¿®æ”¹ï¼ˆneeds_revision: falseï¼‰
- å¦‚æœ80 <= overall_score < 90ï¼šå»ºè®®å¾®è°ƒï¼ˆneeds_revision: trueï¼Œä½†éå¿…é¡»ï¼‰
- å¦‚æœoverall_score < 80ï¼šå¿…é¡»ä¿®æ”¹ï¼ˆneeds_revision: trueï¼‰

## ç¤ºä¾‹
åŸå§‹é—®é¢˜ï¼šåŒ—äº¬3å¤©æ—…è¡Œæ€ä¹ˆå®‰æ’ï¼Ÿ

Thinkerçš„å›ç­”ï¼šæ¨èå»æ•…å®«ã€é•¿åŸã€é¢å’Œå›­ã€‚

è¾“å‡ºï¼š
```json
{{
  "overall_score": 65,
  "dimensions": {
    "completeness": 50,
    "accuracy": 90,
    "relevance": 80,
    "clarity": 90,
    "usefulness": 60
  },
  "issues": [
    {"dimension": "completeness", "issue": "ç¼ºå°‘æ—¶é—´ä¿¡æ¯", "severity": "high"},
    {"dimension": "usefulness", "issue": "ç¼ºå°‘äº¤é€šå’Œä½å®¿å»ºè®®", "severity": "high"}
  ],
  "suggestions": [
    "è¡¥å……å…·ä½“çš„æ—¶é—´å®‰æ’",
    "å¯ä»¥æ·»åŠ æ›´å¤šå¤‡é€‰æ–¹æ¡ˆ"
  ],
  "needs_revision": true,
  "reasoning": "è™½ç„¶æ¨èçš„æ™¯ç‚¹æ˜¯æ­£ç¡®çš„ï¼Œä½†ç”¨æˆ·éœ€è¦çš„æ˜¯è¯¦ç»†çš„è¡Œç¨‹å®‰æ’ï¼Œè€Œä¸ä»…ä»…æ˜¯æ™¯ç‚¹åˆ—è¡¨"
}
```
```

---

## 7. Skillsé›†æˆè®¾è®¡

### 7.1 Skills Engineæ¶æ„

#### 7.1.1 SkillåŸºç±»è®¾è®¡

```python
class Skill:
    """
    SkillåŸºç±»
    """
    def __init__(self, name, description, config):
        self.name = name
        self.description = description
        self.config = config
        self.metadata = {
            "version": config.get("version", "1.0.0"),
            "author": config.get("author", "unknown"),
            "latency_target_ms": config.get("latency_target_ms", 1000),
            "max_retries": config.get("max_retries", 3),
            "timeout_ms": config.get("timeout_ms", 30000),
            "requires_api_key": config.get("requires_api_key", False),
            "required_params": config.get("required_params", []),
        }
    
    async def execute(self, params, context):
        """
        æ‰§è¡ŒSkill
        """
        raise NotImplementedError
    
    async def validate_params(self, params):
        """
        éªŒè¯å‚æ•°
        """
        required = self.metadata["required_params"]
        for param in required:
            if param not in params:
                raise ValueError(f"Missing required parameter: {param}")
        return True
    
    def get_schema(self):
        """
        è·å–å‚æ•°schema
        """
        return {
            "type": "object",
            "properties": {
                param: {
                    "type": param_type,
                    "description": param_description
                }
                for param, param_type, param_description in self._get_param_descriptions()
            },
            "required": self.metadata["required_params"]
        }
    
    def _get_param_descriptions(self):
        """å­ç±»å®ç°"""
        return []
```

#### 7.1.2 Skillç¤ºä¾‹

```python
class WeatherSkill(Skill):
    """
    å¤©æ°”æŸ¥è¯¢Skill
    """
    def __init__(self, config):
        super().__init__(
            name="get_weather",
            description="æŸ¥è¯¢æŒ‡å®šåœ°ç‚¹å’Œæ—¥æœŸçš„å¤©æ°”",
            config=config
        )
        self.api_key = os.getenv("WEATHER_API_KEY")
    
    async def execute(self, params, context):
        location = params.get("location")
        date = params.get("date", "today")
        
        # è°ƒç”¨å¤©æ°”API
        weather_data = await self._call_weather_api(location, date)
        
        # æ ¼å¼åŒ–è¾“å‡º
        result = self._format_weather(weather_data)
        return {
            "success": True,
            "data": weather_data,
            "formatted": result,
            "latency_ms": self._get_elapsed_time()
        }
    
    def _format_weather(self, weather_data):
        return f"{weather_data['location']}ä»Šå¤©{weather_data['condition']}ï¼Œæ°”æ¸©{weather_data['temp_min']}-{weather_data['temp_max']}åº¦"
    
    def _call_weather_api(self, location, date):
        # å®é™…APIè°ƒç”¨
        # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        return {
            "location": location,
            "date": date,
            "condition": "æ™´",
            "temp_min": 18,
            "temp_max": 26
        }
    
    def get_schema(self):
        return {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "åŸå¸‚åç§°"
                },
                "date": {
                    "type": "string",
                    "description": "æ—¥æœŸï¼ˆä»Šå¤©/æ˜å¤©/æˆ–YYYY-MM-DDï¼‰",
                    "default": "today"
                }
            },
            "required": ["location"]
        }
```

### 7.2 Skills Engineå®ç°

#### 7.2.1 Skillæ³¨å†Œä¸å‘ç°

```python
class SkillsEngine:
    def __init__(self):
        self.skills = {}
        self.skill_groups = defaultdict(list)
    
    def register_skill(self, skill):
        """
        æ³¨å†ŒSkill
        """
        self.skills[skill.name] = skill
        group = skill.config.get("group", "general")
        self.skill_groups[group].append(skill.name)
    
    def get_skill(self, skill_name):
        """
        è·å–Skill
        """
        return self.skills.get(skill_name)
    
    def list_skills(self, group=None):
        """
        åˆ—å‡ºæ‰€æœ‰Skills
        """
        if group:
            return [self.skills[name] for name in self.skill_groups[group]]
        return list(self.skills.values())
    
    def search_skills(self, query):
        """
        æœç´¢Skillsï¼ˆåŸºäºæè¿°ï¼‰
        """
        query_lower = query.lower()
        results = []
        for skill in self.skills.values():
            if (query_lower in skill.name.lower() or
                query_lower in skill.description.lower()):
                results.append(skill)
        return results
```

#### 7.2.2 Skillè°ƒç”¨å™¨

```python
class SkillInvoker:
    def __init__(self, skills_engine, cache=None):
        self.engine = skills_engine
        self.cache = cache or SkillCache()
    
    async def invoke(self, skill_name, params, context):
        """
        è°ƒç”¨Skill
        """
        # 1. è·å–Skill
        skill = self.engine.get_skill(skill_name)
        if not skill:
            raise SkillNotFoundError(skill_name)
        
        # 2. éªŒè¯å‚æ•°
        await skill.validate_params(params)
        
        # 3. æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(skill_name, params)
        cached_result = await self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # 4. æ‰§è¡ŒSkillï¼ˆå¸¦é‡è¯•ï¼‰
        result = await self._execute_with_retry(skill, params, context)
        
        # 5. ç¼“å­˜ç»“æœ
        await self.cache.set(cache_key, result)
        
        return result
    
    async def _execute_with_retry(self, skill, params, context, max_retries=3):
        """
        å¸¦é‡è¯•çš„æ‰§è¡Œ
        """
        for attempt in range(max_retries):
            try:
                # è®¾ç½®è¶…æ—¶
                result = await asyncio.wait_for(
                    skill.execute(params, context),
                    timeout=skill.metadata["timeout_ms"] / 1000
                )
                return result
            except asyncio.TimeoutError:
                if attempt == max_retries - 1:
                    raise SkillTimeoutError(skill.name)
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
            except Exception as e:
                if attempt == max_retries - 1:
                    raise SkillExecutionError(skill.name, e)
                await asyncio.sleep(2 ** attempt)
    
    def _generate_cache_key(self, skill_name, params):
        """
        ç”Ÿæˆç¼“å­˜é”®
        """
        return f"{skill_name}:{hash(json.dumps(params, sort_keys=True))}"
```

### 7.3 åŠ¨æ€SkillåŠ è½½

```python
class DynamicSkillLoader:
    def __init__(self, skills_engine):
        self.engine = skills_engine
        self.loaded_skills = {}
    
    async def load_skill_from_config(self, config_path):
        """
        ä»é…ç½®æ–‡ä»¶åŠ è½½Skill
        """
        config = await self._load_config(config_path)
        
        skill_class = self._import_skill_class(config["class"])
        skill = skill_class(config)
        
        self.engine.register_skill(skill)
        self.loaded_skills[skill.name] = config
        
        return skill
    
    async def unload_skill(self, skill_name):
        """
        å¸è½½Skill
        """
        if skill_name in self.loaded_skills:
            del self.engine.skills[skill_name]
            del self.loaded_skills[skill_name]
            return True
        return False
    
    def _import_skill_class(self, class_path):
        """
        å¯¼å…¥Skillç±»
        """
        module_path, class_name = class_path.rsplit(".", 1)
        module = importlib.import_module(module_path)
        return getattr(module, class_name)
```

---

## 8. æ€§èƒ½ä¼˜åŒ–ä¸å»¶è¿Ÿæ§åˆ¶

### 8.1 åˆ†å±‚å“åº”æœºåˆ¶

#### 8.1.1 å“åº”å±‚æ¬¡å®šä¹‰

```python
class ResponseLayer:
    L1_INSTANT = "L1"  # < 100ms: ç›´æ¥å£è¯­å›åº”
    L2_FAST = "L2"      # < 300ms: è½»é‡çº§æ¨¡å‹
    L3_ASYNC = "L3"    # å¼‚æ­¥ï¼šThinkeråå°å¤„ç†
    L4_STREAM = "L4"    # æµå¼ï¼šå®æ—¶æ’­æŠ¥
    
    # å»¶è¿Ÿé˜ˆå€¼
    THRESHOLDS = {
        L1_INSTANT: 100,
        L2_FAST: 300,
        L3_ASYNC: float('inf'),
        L4_STREAM: float('inf')
    }
```

#### 8.1.2 è‡ªé€‚åº”å“åº”ç­–ç•¥

```python
class AdaptiveResponder:
    def __init__(self):
        self.layer = ResponseLayer.L1_INSTANT
        self.latency_target = ResponseLayer.THRESHOLDS[self.layer]
    
    def select_response_layer(self, task_complexity, user_state):
        """
        æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å’Œç”¨æˆ·çŠ¶æ€é€‰æ‹©å“åº”å±‚æ¬¡
        """
        # 1. åŸºäºå¤æ‚åº¦é€‰æ‹©
        if task_complexity == "simple":
            self.layer = ResponseLayer.L1_INSTANT
        elif task_complexity == "medium":
            self.layer = ResponseLayer.L2_FAST
        else:  # complex
            self.layer = ResponseLayer.L3_ASYNC
        
        # 2. åŸºäºç”¨æˆ·çŠ¶æ€è°ƒæ•´
        if user_state.get("patience_level", "high") == "low":
            # ç”¨æˆ·ä¸è€çƒ¦ï¼Œæå‡å“åº”é€Ÿåº¦
            if self.layer == ResponseLayer.L3_ASYNC:
                self.layer = ResponseLayer.L2_FAST
        
        return self.layer
    
    async def respond(self, task, user_state):
        """
        è‡ªé€‚åº”å“åº”
        """
        layer = self.select_response_layer(
            task.get("complexity", "simple"),
            user_state
        )
        
        if layer == ResponseLayer.L1_INSTANT:
            # L1: ç›´æ¥å£è¯­å›åº”
            return await self.l1_respond(task)
        elif layer == ResponseLayer.L2_FAST:
            # L2: è½»é‡çº§æ¨¡å‹
            return await self.l2_respond(task)
        elif layer == ResponseLayer.L3_ASYNC:
            # L3: å¼‚æ­¥å¤„ç†
            return await self.l3_respond(task)
        elif layer == ResponseLayer.L4_STREAM:
            # L4: æµå¼æ’­æŠ¥
            return self.l4_respond(task)
```

### 8.2 æµå¼è¾“å‡ºä¼˜åŒ–

```python
class StreamingResponder:
    def __init__(self):
        self.chunk_size = 50  # æ¯æ¬¡ç”Ÿæˆçš„tokenæ•°é‡
        self.delay = 0.05   # ç”Ÿæˆchunkä¹‹é—´çš„å»¶è¿Ÿ
    
    async def stream_response(self, prompt, model="gpt-4.1-mini"):
        """
        æµå¼ç”Ÿæˆå“åº”
        """
        # 1. LLMè°ƒç”¨ï¼ˆæµå¼ï¼‰
        async for chunk in llm_stream_generate(prompt, model):
            yield chunk
            
            # 2. æ·»åŠ å»¶è¿Ÿæ¨¡æ‹Ÿ"è¾¹æƒ³è¾¹è¯´"
            if self._should_add_think_pause():
                yield "[æ€è€ƒä¸­...]"
                await asyncio.sleep(self.delay * 5)
    
    def _should_add_think_pause(self):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ·»åŠ "æ€è€ƒä¸­"æš‚åœ
        """
        # ç®€å•è§„åˆ™ï¼šæ¯éš”10ä¸ªtokenæš‚åœä¸€æ¬¡
        return random.random() < 0.1
```

### 8.3 å¹¶å‘ä¼˜åŒ–

```python
class ConcurrentExecutor:
    def __init__(self, max_concurrent=10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def execute_concurrent(self, tasks):
        """
        å¹¶å‘æ‰§è¡Œå¤šä¸ªä»»åŠ¡
        """
        results = await asyncio.gather(*[
            self._execute_with_semaphore(task)
            for task in tasks
        ])
        return results
    
    async def _execute_with_semaphore(self, task):
        """
        ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        """
        async with self.semaphore:
            return await task()
    
    async def execute_parallel_independent(self, tasks):
        """
        å¹¶å‘æ‰§è¡Œç‹¬ç«‹ä»»åŠ¡
        """
        results = await self.execute_concurrent(tasks)
        return results
    
    async def execute_parallel_sequential(self, task_groups):
        """
        å¹¶å‘æ‰§è¡Œä»»åŠ¡ç»„ï¼ˆç»„é—´å¹¶è¡Œï¼Œç»„å†…é¡ºåºï¼‰
        """
        results = []
        for group in task_groups:
            group_results = await self.execute_concurrent(group)
            results.extend(group_results)
        return results
```

---

## 9. ç”¨æˆ·ä½“éªŒè®¾è®¡

### 9.1 å®æ—¶æ„ŸçŸ¥ä¼˜åŒ–

#### 9.1.1 è¿›åº¦å¯è§†åŒ–

```python
class ProgressIndicator:
    def __init__(self):
        self.current_progress = 0
        self.total_steps = 1
        self.start_time = time.time()
    
    def update_progress(self, current, total):
        """
        æ›´æ–°è¿›åº¦
        """
        self.current_progress = current
        self.total_steps = total
        self.elapsed = time.time() - self.start_time
        
        # ä¼°ç®—å‰©ä½™æ—¶é—´
        if self.current_progress > 0:
            avg_time_per_step = self.elapsed / self.current_progress
            self.estimated_remaining = avg_time_per_step * (self.total_steps - self.current_progress)
        else:
            self.estimated_remaining = 0
    
    def get_progress_message(self):
        """
        ç”Ÿæˆè¿›åº¦æ¶ˆæ¯
        """
        if self.total_steps == 1:
            return "æ­£åœ¨å¤„ç†ä¸­..."
        
        percentage = int(self.current_progress / self.total_steps * 100)
        bar = self._generate_progress_bar(percentage)
        
        messages = [
            f"[è¿›åº¦] {percentage}%",
            f"{bar}",
            f"({self.current_progress}/{self.total_steps}æ­¥éª¤)",
            f"é¢„è®¡å‰©ä½™: {self._format_time(self.estimated_remaining)}"
        ]
        
        return " ".join(messages)
    
    def _generate_progress_bar(self, percentage, width=20):
        filled = int(percentage / 100 * width)
        return "[" + "=" * filled + " " " * (width - filled) + "]"
    
    def _format_time(self, seconds):
        if seconds < 60:
            return f"{int(seconds)}ç§’"
        elif seconds < 3600:
            return f"{int(seconds / 60)}åˆ†é’Ÿ"
        else:
            return f"{int(seconds / 3600)}å°æ—¶"
```

#### 9.1.2 çŠ¶æ€é€šçŸ¥

```python
class StatusNotifier:
    def __init__(self):
        self.current_status = "idle"
        self.status_history = []
    
    async def notify_status_change(self, old_status, new_status):
        """
        é€šçŸ¥çŠ¶æ€å˜åŒ–
        """
        self.current_status = new_status
        self.status_history.append({
            "old_status": old_status,
            "new_status": new_status,
            "timestamp": time.time()
        })
        
        # æ ¹æ®çŠ¶æ€å˜åŒ–å‘é€é€šçŸ¥
        await self._send_notification(old_status, new_status)
    
    async def _send_notification(self, old_status, new_status):
        """
        å‘é€é€šçŸ¥
        """
        if self._should_notify(old_status, new_status):
            message = self._format_status_message(new_status)
            await send_to_user(message)
    
    def _should_notify(self, old_status, new_status):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥é€šçŸ¥
        """
        # çŠ¶æ€ä»"working"å˜åˆ°"completed"æ—¶ï¼Œé€šçŸ¥
        return old_status == "working" and new_status == "completed"
    
    def _format_status_message(self, status):
        """
        æ ¼å¼åŒ–çŠ¶æ€æ¶ˆæ¯
        """
        messages = {
            "idle": "ğŸ¯ å‡†å¤‡å°±ç»ª",
            "working": "ğŸ”„ æ­£åœ¨å¤„ç†...",
            "thinking": "ğŸ¤” æ­£åœ¨æ·±åº¦æ€è€ƒ...",
            "completed": "âœ… å¤„ç†å®Œæˆ",
            "error": "âŒ å¤„ç†å‡ºé”™",
            "paused": "â¸ï¸ å·²æš‚åœ"
        }
        return messages.get(status, f"[{status}]")
```

### 9.2 ä¸»åŠ¨å¯¹è¯ç­–ç•¥

```python
class ProactiveConversationalist:
    def __init__(self):
        self.last_interaction = time.time()
        self.idle_threshold = 30  # 30ç§’æ— äº¤äº’è§†ä¸ºç©ºé—²
        self.last_topics = []
    
    async def check_and_initiate(self, conversation_context):
        """
        æ£€æŸ¥å¹¶ä¸»åŠ¨å‘èµ·å¯¹è¯
        """
        idle_time = time.time() - self.last_interaction
        
        if idle_time < self.idle_threshold:
            return "NO_ACTION"
        
        # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨å‘èµ·
        if self._should_initiate(conversation_context):
            topic = self._generate_topic(conversation_context)
            self.last_interaction = time.time()
            return topic
        else:
            return "NO_ACTION"
    
    def _should_initiate(self, context):
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨å‘èµ·
        """
        # ä¸åœ¨ä»¥ä¸‹æƒ…å†µå‘èµ·ï¼š
        if self._is_in_important_conversation(context):
            return False
        if self._is_recent_question_answered(context):
            return False
        if self._is_late_night():
            return False
        
        return True
    
    def _generate_topic(self, context):
        """
        ç”Ÿæˆè¯é¢˜
        """
        # 1. åˆ†æä¸Šä¸‹æ–‡
        recent_messages = context.get("messages", [])[-5:]
        user_interests = context.get("user_interests", {})
        
        # 2. åŸºäºç”¨æˆ·å…´è¶£ç”Ÿæˆè¯é¢˜
        if "technology" in user_interests:
            topic = "å¯¹äº†ï¼Œæˆ‘æœ€è¿‘çœ‹åˆ°ä¸€ç¯‡å…³äºAIæŠ€æœ¯çš„æ–‡ç« ï¼ŒæŒºæœ‰æ„æ€çš„..."
        elif "travel" in user_interests:
            topic = "é¡ºä¾¿é—®ä¸€ä¸‹ï¼Œä½ æœ€è¿‘æœ‰æ—…è¡Œè®¡åˆ’å—ï¼Ÿ"
        else:
            topic = "æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿ"
        
        return topic
    
    def record_interaction(self):
        """
        è®°å½•äº¤äº’
        """
        self.last_interaction = time.time()
```

---

## 10. å·¥ç¨‹å®ç°æœ€ä½³å®è·µ

### 10.1 ä»£ç ç»“æ„è®¾è®¡

```
dual-agent-system/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ talker/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.md
â”‚   â”‚   â”‚   â”œâ”€â”€ classification.md
â”‚   â”‚   â”‚   â”œâ”€â”€ progress.md
â”‚   â”‚   â”‚   â””â”€â”€ proactive.md
â”‚   â”‚   â””â”€â”€ skills/
â”‚   â”‚       â””â”€â”€ __init__.py
â”‚   â””â”€â”€ thinker/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ agent.py
â”‚       â”œâ”€â”€ planner.py
â”‚       â”œâ”€â”€ prompts/
â”‚       â”‚   â”œâ”€â”€ planning.md
â”‚       â”‚   â”œâ”€â”€ execution.md
â”‚       â”‚   â””â”€â”€ reflection.md
â”‚       â””â”€â”€ skills/
â”‚           â””â”€â”€ __init__.py
â”œâ”€â”€ orchestrator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coordinator.py
â”‚   â”œâ”€â”€ scheduler.py
â”‚   â””â”€â”€ handoff.py
â”œâ”€â”€ context/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ working_context.py
â”‚   â”œâ”€â”€ session_context.py
â”‚   â”œâ”€â”€ long_term_memory.py
â”‚   â””â”€â”€ knowledge_base.py
â”œâ”€â”€ skills/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py
â”‚   â”œâ”€â”€ engine.py
â”‚   â”œâ”€â”€ invoker.py
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ weather.py
â”‚       â”œâ”€â”€ search.py
â”‚       â””â”€â”€ calculation.py
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â””â”€â”€ alerts.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ prompts/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_talker.py
â”‚   â”œâ”€â”€ test_thinker.py
â”‚   â””â”€â”€ test_orchestrator.py
â””â”€â”€ main.py
```

### 10.2 é…ç½®ç®¡ç†

```python
# config/settings.py
class Settings:
    # Agenté…ç½®
    TALKER_MODEL = os.getenv("TALKER_MODEL", "gpt-4.1-mini")
    THINKER_MODEL = os.getenv("THINKER_MODEL", "gpt-4.1")
    
    # è¶…æ—¶é…ç½®
    TALKER_TIMEOUT_MS = 500
    THINKER_TIMEOUT_MS = 30000
    SKILL_TIMEOUT_MS = 10000
    
    # ç¼“å­˜é…ç½®
    CACHE_TTL_SECONDS = 3600
    REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
    
    # å¹¶å‘é…ç½®
    MAX_CONCURRENT_TASKS = 10
    MAX_CONCURRENT_SKILLS = 5
    
    # ä¸Šä¸‹æ–‡é…ç½®
    MAX_CONTEXT_MESSAGES = 100
    MAX_SESSION_HISTORY = 1000
    WORKING_CONTEXT_MESSAGES = 10
    
    # ç›‘æ§é…ç½®
    ENABLE_METRICS = True
    LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
    ALERT_WEBHOOK_URL = os.getenv("ALERT_WEBHOOK_URL")
    
    # ç‰¹æ€§å¼€å…³
    ENABLE_STREAMING = True
    ENABLE_PROGRESS_NOTIFICATION = True
    ENABLE_PROACTIVE_CONVERSATION = True
```

### 10.3 éƒ¨ç½²æ–¹æ¡ˆ

#### 10.3.1 Dockeréƒ¨ç½²

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1

# å¯åŠ¨å‘½ä»¤
CMD ["python", "main.py"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  dual-agent:
    build: .
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis
  
  redis:
    image: redis:7
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus
    depends_on:
      - dual-agent
```

#### 10.3.2 Kuberneteséƒ¨ç½²

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dual-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dual-agent
  template:
    metadata:
      labels:
        app: dual-agent
    spec:
      containers:
      - name: dual-agent
        image: dual-agent:latest
        ports:
          - containerPort: 8000
        env:
          - name: REDIS_URL
            valueFrom:
              secretKeyRef:
                name: redis-url
          - name: OPENAI_API_KEY
            valueFrom:
              secretKeyRef:
                name: openai-api-key
        resources:
          limits:
            cpu: "1000m"
            memory: "2Gi"
          requests:
            cpu: "500m"
            memory: "1Gi"
---
apiVersion: v1
kind: Service
metadata:
  name: dual-agent
spec:
  selector:
    matchLabels:
      app: dual-agent
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
```

---

## 11. ç›‘æ§ä¸è¿ç»´

### 11.1 æŒ‡æ ‡æ”¶é›†

```python
class MetricsCollector:
    def __init__(self, prometheus_client):
        self.prometheus = prometheus_client
        
        # è®¡æ•°å™¨
        self.request_counter = prometheus_client.Counter(
            'dual_agent_requests_total',
            'Total requests received',
            ['agent', 'complexity', 'layer']
        )
        
        # å»¶è¿Ÿç›´æ–¹å›¾
        self.latency_histogram = prometheus_client.Histogram(
            'dual_agent_latency_seconds',
            'Request latency in seconds',
            ['agent', 'layer', 'model']
        )
        
        # æˆåŠŸç‡ä»ªè¡¨æ¿
        self.success_rate_gauge = prometheus_client.Gauge(
            'dual_agent_success_rate',
            'Request success rate',
            ['agent']
        )
    
    async def record_request(self, agent, complexity, layer, model, latency, success):
        """è®°å½•è¯·æ±‚"""
        self.request_counter.labels(
            agent=agent,
            complexity=complexity,
            layer=layer
        ).inc()
        
        self.latency_histogram.labels(
            agent=agent,
            layer=layer,
            model=model
        ).observe(latency)
        
        if success:
            self.success_rate_gauge.labels(agent=agent).set(1.0)
        else:
            self.success_rate_gauge.labels(agent=agent).set(0.0)
    
    async def record_handoff(self, from_agent, to_agent, reason):
        """è®°å½•handoff"""
        self.request_counter.labels(
            agent="handoff",
            complexity="N/A",
            layer="N/A"
        ).inc()
```

### 11.2 æ—¥å¿—è®°å½•

```python
import logging
import sys

class StructuredLogger:
    def __init__(self, name):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.INFO)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s - %(message)s'
        ))
        self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(f'/var/log/{name}.log')
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s [%(levelname)s] %(name)s - %(message)s'
        ))
        self.logger.addHandler(file_handler)
    
    def log_request(self, request_id, agent, task, start_time, end_time=None, status="completed", error=None):
        """è®°å½•è¯·æ±‚"""
        duration = (end_time or time.time()) - start_time
        
        log_data = {
            "request_id": request_id,
            "agent": agent,
            "task": str(task),
            "duration": duration,
            "status": status,
            "error": str(error) if error else None
        }
        
        if status == "completed":
            self.logger.info(f"Request completed: {log_data}")
        elif status == "error":
            self.logger.error(f"Request error: {log_data}")
        else:
            self.logger.info(f"Request status: {log_data}")
```

### 11.3 å‘Šè­¦æœºåˆ¶

```python
class AlertManager:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url
        self.alert_rules = {
            "high_latency": {
                "threshold": 5.0,  # 5ç§’
                "condition": "latency > threshold",
                "severity": "warning"
            },
            "high_error_rate": {
                "threshold": 0.1,  # 10%
                "condition": "error_rate > threshold",
                "severity": "critical"
            },
            "skill_failure": {
                "condition": "skill_failure_count > 3 in last_5_minutes",
                "severity": "warning"
            }
        }
    
    async def check_and_alert(self, metrics):
        """æ£€æŸ¥æŒ‡æ ‡å¹¶å‘é€å‘Šè­¦"""
        alerts = []
        
        # æ£€æŸ¥å»¶è¿Ÿ
        for agent in ["talker", "thinker"]:
            latency = metrics.get_agent_latency(agent, "p95")
            if latency > self.alert_rules["high_latency"]["threshold"]:
                alerts.append({
                    "type": "high_latency",
                    "agent": agent,
                    "value": latency,
                    "threshold": self.alert_rules["high_latency"]["threshold"],
                    "severity": self.alert_rules["high_latency"]["severity"]
                })
        
        # æ£€æŸ¥é”™è¯¯ç‡
        for agent in ["talker", "thinker"]:
            error_rate = metrics.get_agent_error_rate(agent)
            if error_rate > self.alert_rules["high_error_rate"]["threshold"]:
                alerts.append({
                    "type": "high_error_rate",
                    "agent": agent,
                    "value": error_rate,
                    "threshold": self.alert_rules["high_error_rate"]["threshold"],
                    "severity": self.alert_rules["high_error_rate"]["severity"]
                })
        
        # å‘é€å‘Šè­¦
        if alerts:
            await self._send_webhook(alerts)
    
    async def _send_webhook(self, alerts):
        """å‘é€webhook"""
        payload = {
            "timestamp": time.time(),
            "alerts": alerts
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.webhook_url,
                json=payload,
                headers={"Content-Type": "application/json"}
            ) as response:
                if response.status == 200:
                    print(f"Alerts sent successfully")
                else:
                    print(f"Failed to send alerts: {response.status}")
```

---

## 12. ä¸šç•Œæœ€ä½³å®è·µå¯¹æ¯”

### 12.1 ä¸»æµæ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | Talkeræ”¯æŒ | Thinkeræ”¯æŒ | Handoffæœºåˆ¶ | æµå¼è¾“å‡º | å¼€æºçŠ¶æ€ |
|------|-----------|--------------|-------------|----------|----------|
| **AutoGen** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **LangGraph** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **Claude Agent Teams** | âœ… | âœ… | âœ… | âœ… | âŒ |
| **Mini-Omni-Reasoner** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **æˆ‘ä»¬çš„æ¶æ„** | âœ… | âœ… | âœ… | âœ… | âœ… |

### 12.2 æ ¸å¿ƒä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | AutoGen | LangGraph | Claude Teams | Mini-Omni | æˆ‘ä»¬çš„æ¶æ„ |
|------|---------|-----------|-------------|---------------|-----------|
| **Token-Level Interleaved** | âŒ | âŒ | âŒ | âœ… | âœ… |
| **è‡ªé€‚åº”å“åº”å±‚æ¬¡** | âŒ | âŒ | âŒ | âŒ | âœ… |
| **Think-in-Speaking** | âŒ | âŒ | âŒ | âœ… | âœ… |
| **æ¸è¿›å¼ä¸Šä¸‹æ–‡å‹ç¼©** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **å¤šå±‚ç¼“å­˜** | âœ… | âœ… | âœ… | âœ… | âœ… |
| **å®æ—¶è¿›åº¦æ’­æŠ¥** | âœ… | âœ… | âœ… | âœ… | âœ… |

### 12.3 å®é™…åº”ç”¨åœºæ™¯å¯¹æ¯”

| åœºæ™¯ | AutoGen | LangGraph | Claude Teams | Mini-Omni | æˆ‘ä»¬çš„æ¶æ„ |
|------|---------|-----------|-------------|---------------|-----------|
| **å¿«é€Ÿé—®ç­”** | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **å¤æ‚æ¨ç†** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **è¯­éŸ³äº¤äº’** | âŒ | âŒ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **å®æ—¶æ€è€ƒ** | âŒ | âŒ | âŒ | â­â­â­â­â­â­â­ | â­â­â­â­â­ |
| **ä½å»¶è¿Ÿ** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

---

## 13. æœªæ¥ç ”ç©¶æ–¹å‘

### 13.1 æŠ€æœ¯åˆ›æ–°æ–¹å‘

1. **æ›´ç²¾ç»†çš„äº¤é”™ç”Ÿæˆç­–ç•¥**ï¼š
   - åŸºäºè¯­ä¹‰ç†è§£çš„æ™ºèƒ½äº¤é”™
   - åŠ¨æ€è°ƒæ•´Thinkingå’ŒSpeakingæ¯”ä¾‹
   - è·¨æ¨¡æ€çš„äº¤é”™ç”Ÿæˆï¼ˆæ–‡æœ¬+è¯­éŸ³+è§†é¢‘ï¼‰

2. **è‡ªé€‚åº”ä¸Šä¸‹æ–‡ç®¡ç†**ï¼š
   - åŸºäºç”¨æˆ·è¡Œä¸ºåŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡
   - ä¸ªæ€§åŒ–ä¸Šä¸‹æ–‡å‹ç¼©ç­–ç•¥
   - è·¨ä¼šè¯çš„ä¸Šä¸‹æ–‡å¤ç”¨

3. **å¤šAgentåä½œåè®®æ ‡å‡†åŒ–**ï¼š
   - æ ‡å‡†åŒ–çš„Handoffåè®®
   - é€šç”¨çš„Agenté€šä¿¡åè®®
   - ç»Ÿä¸€çš„ä¸Šä¸‹æ–‡æ ¼å¼

### 13.2 æ€§èƒ½ä¼˜åŒ–æ–¹å‘

1. **æ¨¡å‹è’¸é¦ä¸é‡åŒ–**ï¼š
   - å°†Thinkerçš„èƒ½åŠ›è’¸é¦åˆ°Talker
   - å‡å°‘å¯¹å¼ºå¤§æ¨¡å‹çš„ä¾èµ–
   - é™ä½æˆæœ¬å’Œå»¶è¿Ÿ

2. **é¢„æµ‹æ€§é¢„è®¡ç®—**ï¼š
   - é¢„è®¡ç®—å¸¸è§é—®é¢˜çš„ç­”æ¡ˆ
   - ç¼“å­˜æ¨ç†è·¯å¾„
   - æµæ°´çº¿ä¼˜åŒ–

3. **è¾¹ç¼˜è®¡ç®—ä¼˜åŒ–**ï¼š
   - åœ¨è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²è½»é‡çº§Talker
   - äº‘ç«¯è¿è¡ŒThinker
   - æ··åˆæ¶æ„

### 13.3 åº”ç”¨æ‰©å±•æ–¹å‘

1. **å¤šæ¨¡æ€æ‰©å±•**ï¼š
   - å›¾åƒç†è§£é›†æˆ
   - è§†é¢‘å¤„ç†
   - å¤šæ¨¡æ€ç»Ÿä¸€ä¸Šä¸‹æ–‡

2. **ä¸ªæ€§åŒ–å®šåˆ¶**ï¼š
   - ç”¨æˆ·åå¥½å­¦ä¹ 
   - ä¸ªæ€§åŒ–Prompt
   - è‡ªé€‚åº”äº¤äº’é£æ ¼

3. **é¢†åŸŸçŸ¥è¯†æ•´åˆ**ï¼š
   - ç‰¹å®šé¢†åŸŸçŸ¥è¯†åº“
   - RAGä¼˜åŒ–
   - çŸ¥è¯†å›¾è°±

---

## 14. å‚è€ƒæ–‡çŒ®

### 14.1 æ ¸å¿ƒè®ºæ–‡

1. **Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models**
   - **arXiv**ï¼šhttps://arxiv.org/abs/2508.15827
   - **GitHub**ï¼šhttps://github.com/xzf-thu/Mini-Omni-Reasoner
   - **å‘è¡¨æ—¶é—´**ï¼š2025å¹´8æœˆ

2. **2410.08328v1.pdf**
   - [éœ€é€šè¿‡PDFè§£æç¡®å®šæ ‡é¢˜å’Œå†…å®¹]
   - [å¾…è¡¥å……å®Œæ•´å¼•ç”¨ä¿¡æ¯]

### 14.2 ç›¸å…³å·¥ä½œ

1. **AutoGen: Enabling LLM Applications with Multi-Agent Conversation**
   - **arXiv**ï¼šhttps://arxiv.org/abs/2308.08155
   - **GitHub**ï¼šhttps://github.com/microsoft/autogen
   - **å‘è¡¨æ—¶é—´**ï¼š2023å¹´8æœˆ

2. **ChatDev: Communicative Agents for Software Development**
   - **arXiv**ï¼šhttps://arxiv.org/abs/2307.07924
   - **GitHub**ï¼šhttps://github.com/OpenBMB/ChatDev

3. **CAMEL: Communicative Agents for 'Mind' Exploration of Large Scale Language Model Society**
   - **arXiv**ï¼šhttps://arxiv.org/abs/2303.17760
   - **GitHub**ï¼šhttps://github.com/camel-lab/CAMEL

4. **LangGraph: LangGraph: A Framework for Stateful Multi-Agent Applications**
   - **GitHub**ï¼šhttps://github.com/langchain-ai/langgraph
   - **æ–‡æ¡£**ï¼šhttps://langgraph-ai.readthedocs.io/

5. **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models**
   - **arXiv**ï¼šhttps://arxiv.org/abs/2201.11903
   - **å‘è¡¨æ—¶é—´**ï¼š2022å¹´1æœˆ

### 14.3 å®è·µæ–‡ç« 

1. **LLM Multi-Agent Architecture: How AI Teams Work Together**
   - **é“¾æ¥**ï¼šhttps://sam-solutions.com/blog/llm-multi-agent-architecture/
   - **å‘è¡¨æ—¶é—´**ï¼š2025å¹´11æœˆ

2. **Multi-Agent Architecture for Design of WSN Applications**
   - **arXiv**ï¼šhttp://www.scirp.org/journal/PaperInformation.aspx?PaperID=27989
   - **å‘è¡¨æ—¶é—´**ï¼š2013å¹´2æœˆ

3. **Plan Better Amid Conservatism: Offline Multi-Agent RL with Actor Critic**
   - **çŸ¥ä¹**ï¼šhttps://zhuanlan.zhihu.com/p/437097245
   - **å‘è¡¨æ—¶é—´**ï¼š2021å¹´11æœˆ

---

## é™„å½•Aï¼šå¿«é€Ÿå¼€å§‹æŒ‡å—

### A.1 ç¯å¢ƒå‡†å¤‡

```bash
# 1. å®‰è£…Python 3.11+
python3 --version

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-openai-api-key"
export ANTHROPIC_API_KEY="your-anthropic-api-key"
export REDIS_URL="redis://localhost:6379"
```

### A.2 æœ¬åœ°è¿è¡Œ

```bash
# 1. å¯åŠ¨Redis
docker-compose up -d redis

# 2. è¿è¡Œä¸»ç¨‹åº
python main.py
```

### A.3 Dockeréƒ¨ç½²

```bash
# 1. æ„å»ºé•œåƒ
docker build -t dual-agent:latest .

# 2. è¿è¡Œå®¹å™¨
docker-compose up -d
```

### A.4 Kuberneteséƒ¨ç½²

```bash
# 1. åˆ›å»ºconfigmap
kubectl create secret generic redis-url --from-literal=redis://redis:6379
kubectl create secret generic openai-api-key --from-literal=${OPENAI_API_KEY}

# 2. éƒ¨ç½²
kubectl apply -f k8s/deployment.yaml
kubectl apply -f k8s/service.yaml
```

---

## é™„å½•Bï¼šå¸¸è§é—®é¢˜FAQ

### Q1: å¦‚ä½•å†³å®šä»»åŠ¡ç”±Talkerè¿˜æ˜¯Thinkerå¤„ç†ï¼Ÿ

**A**: Orchestratorä¼šåŸºäºä»¥ä¸‹å› ç´ åšå†³ç­–ï¼š
- ä»»åŠ¡å¤æ‚åº¦ï¼ˆç®€å•/å¤æ‚ï¼‰
- é¢„è®¡è€—æ—¶ï¼ˆçŸ­/é•¿ï¼‰
- ç”¨æˆ·ä¼˜å…ˆçº§ï¼ˆç´§æ€¥/æ™®é€šï¼‰
- å½“å‰ç³»ç»Ÿè´Ÿè½½

### Q2: å¦‚ä½•ä¿è¯Talkerå’ŒThinkerçš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼Ÿ

**A**: é€šè¿‡ä»¥ä¸‹æœºåˆ¶ï¼š
- å…±äº«å†…å­˜ä¸Šä¸‹æ–‡ï¼ˆå®æ—¶åŒæ­¥ï¼‰
- äº‹ä»¶æº¯æºï¼ˆå¯è¿½æº¯ï¼‰
- ä¹è§‚é”ï¼ˆé¿å…å†²çªï¼‰
- æœ€ç»ˆä¸€è‡´æ€§åè®®ï¼ˆè§£å†³å†²çªï¼‰

### Q3: å¦‚ä½•å¤„ç†Thinkerå¤±è´¥çš„æƒ…å†µï¼Ÿ

**A**: 
- è¶…æ—¶é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
- é™çº§ç­–ç•¥ï¼ˆä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆï¼‰
- é”™è¯¯æ¢å¤ï¼ˆå›æ»šåˆ°ä¹‹å‰çŠ¶æ€ï¼‰
- ç”¨æˆ·é€šçŸ¥ï¼ˆå‹å¥½æç¤ºï¼‰

### Q4: å¦‚ä½•ä¼˜åŒ–å»¶è¿Ÿï¼Ÿ

**A**:
- ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼ˆTalkerï¼‰
- æµå¼è¾“å‡ºï¼ˆTalkerï¼‰
- å¹¶å‘å¤„ç†ï¼ˆå¤šä¸ªAgentï¼‰
- ç¼“å­˜ç»“æœï¼ˆå‡å°‘è®¡ç®—ï¼‰
- é¢„è®¡ç®—å¸¸è§ç­”æ¡ˆ

### Q5: å¦‚ä½•æ‰©å±•åˆ°è¯­éŸ³äº¤äº’ï¼Ÿ

**A**:
- é›†æˆASRï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰
- ä½¿ç”¨TTSï¼ˆè¯­éŸ³åˆæˆï¼‰
- æ”¯æŒæµå¼éŸ³é¢‘è¾“å…¥/è¾“å‡º
- Talkerè´Ÿè´£TTSè¾“å‡º

---

## é™„å½•Cï¼šæœ¯è¯­è¡¨

| æœ¯è¯­ | è§£é‡Š |
|------|------|
| **Agent** | æ™ºèƒ½ä½“ï¼Œå…·æœ‰è‡ªä¸»å†³ç­–èƒ½åŠ›çš„AIå®ä½“ |
| **Talker** | å¯¹è¯è€…ï¼Œè´Ÿè´£å¿«é€Ÿå“åº”ã€ç®€å•æ„å›¾é—­ç¯ã€å®æ—¶åé¦ˆ |
| **Thinker** | æ€è€ƒè€…ï¼Œè´Ÿè´£å¤æ‚æ¨ç†ã€é•¿ç¨‹è§„åˆ’ã€æ·±åº¦ä»»åŠ¡å¤„ç† |
| **Handoff** | äº¤æ¥ï¼Œå°†ä»»åŠ¡ä»ä¸€ä¸ªAgentè½¬ç§»åˆ°å¦ä¸€ä¸ª |
| **Orchestrator** | åè°ƒå™¨ï¼Œç®¡ç†å¤šä¸ªAgentçš„åä½œ |
| **Context Store** | ä¸Šä¸‹æ–‡å­˜å‚¨ï¼Œå­˜å‚¨å¯¹è¯å†å²å’ŒçŠ¶æ€ |
| **Skill** | æŠ€èƒ½ï¼ŒAgentå¯è°ƒç”¨çš„åŠŸèƒ½å•å…ƒ |
| **LLM** | å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼‰ |
| **RAG** | æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generationï¼‰ |
| **CoT** | é“¾å¼æ€è€ƒï¼ˆChain of Thoughtï¼‰ |
| **TiS** | Thinking-in-Speakingï¼Œè¾¹æ€è€ƒè¾¹è¯´è¯ |
| **Token-Level Interleaved** | Tokençº§åˆ«çš„äº¤é”™ç”Ÿæˆ |

---

**æ–‡æ¡£ç»“æŸ**

*Â© 2026 OpenClaw. æœ¬æ–‡æ¡£åŸºäºä¸šç•Œæœ€ä½³å®è·µå’Œç ”ç©¶ç¼–å†™ï¼Œæ¬¢è¿åé¦ˆå’Œæ”¹è¿›ã€‚*
