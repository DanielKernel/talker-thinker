"""
è¯„æµ‹ç³»ç»Ÿ CLI å…¥å£

ä½¿ç”¨æ–¹å¼:
    python -m evals run                     # è¿è¡Œå…¨éƒ¨è¯„æµ‹
    python -m evals run --category simple   # è¿è¡Œç‰¹å®šç±»åˆ«
    python -m evals report --format html    # ç”ŸæˆæŠ¥å‘Š
"""
import argparse
import asyncio
import sys
import time
from pathlib import Path

from .harness import EvalRunner, EvalConfig
from .cases import get_all_cases, get_cases_by_category
from .core.types import EvalCategory, Priority
from .reporters.console import ConsoleReporter
from .reporters.json_reporter import JSONReporter
from .reporters.html import HTMLReporter


def cmd_run(args):
    """è¿è¡Œè¯„æµ‹"""
    print("ğŸš€ å¼€å§‹è¿è¡Œè¯„æµ‹...")

    # æ„å»ºé…ç½®
    config = EvalConfig(
        use_mock_llm=not args.real_llm,
        show_progress=args.verbose,
        category_filter=None,
    )

    if args.category:
        try:
            config.category_filter = EvalCategory(args.category)
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„ç±»åˆ«ï¼š{args.category}")
            print("æœ‰æ•ˆç±»åˆ«ï¼šsimple, medium, complex, edge")
            sys.exit(1)

    if args.timeout:
        config.case_timeout_seconds = args.timeout

    if args.latency:
        config.mock_talker_latency_ms = args.latency

    # åˆ›å»º Runner å¹¶è¿è¡Œ
    runner = EvalRunner(config)

    try:
        eval_result = asyncio.run(runner.run())
    except KeyboardInterrupt:
        print("\nâš ï¸  è¯„æµ‹è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ è¯„æµ‹æ‰§è¡Œå¤±è´¥ï¼š{e}")
        sys.exit(1)

    # è¾“å‡ºç»“æœ
    if args.verbose:
        reporter = ConsoleReporter(verbose=True)
        reporter.print(eval_result)
    else:
        # ç®€æ´è¾“å‡º
        print(f"\nğŸ“Š è¯„æµ‹å®Œæˆ")
        print(f"   æ€»ç”¨ä¾‹ï¼š{eval_result.total_cases}")
        print(f"   é€šè¿‡ï¼š{eval_result.passed_cases} ({eval_result.pass_rate:.1f}%)")
        print(f"   å¤±è´¥ï¼š{eval_result.failed_cases}")
        print(f"   å¹³å‡å¾—åˆ†ï¼š{eval_result.average_score:.1f}")
        print(f"   å¹³å‡å“åº”æ—¶é—´ï¼š{eval_result.average_response_time:.1f}ms")

    # å¯¼å‡ºç»“æœ
    if args.output:
        # JSON å¯¼å‡º
        json_reporter = JSONReporter()
        json_path = json_reporter.export(eval_result, args.output)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°ï¼š{json_path}")

    if args.html:
        html_reporter = HTMLReporter()
        html_path = args.html
        if not html_path.endswith(".html"):
            html_path = html_path + ".html"
        html_reporter.export(eval_result, html_path)
        print(f"ğŸ“„ HTML æŠ¥å‘Šå·²ç”Ÿæˆï¼š{html_path}")

    # è¿”å›é€€å‡ºç 
    sys.exit(0 if eval_result.pass_rate >= 85 else 1)


def cmd_list(args):
    """åˆ—å‡ºè¯„æµ‹ç”¨ä¾‹"""
    if args.category:
        try:
            category = EvalCategory(args.category)
            cases = get_cases_by_category(category)
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„ç±»åˆ«ï¼š{args.category}")
            print("æœ‰æ•ˆç±»åˆ«ï¼šsimple, medium, complex, edge")
            sys.exit(1)
    else:
        cases = get_all_cases()

    print(f"ğŸ“‹ è¯„æµ‹ç”¨ä¾‹åˆ—è¡¨ (å…± {len(cases)} ä¸ª)\n")

    current_category = None
    for case in cases:
        if case.category != current_category:
            current_category = case.category
            print(f"\n{'='*60}")
            print(f"ã€{category_name(current_category)}ã€‘")
            print(f"{'='*60}")

        priority_mark = {
            Priority.CRITICAL: "ğŸ”´",
            Priority.HIGH: "ğŸŸ¡",
            Priority.NORMAL: "ğŸŸ¢",
            Priority.LOW: "âšª",
        }.get(case.priority, "")

        print(f"  {priority_mark} {case.case_id}: {case.name}")
        print(f"      {case.description}")
        print(f"      æœŸæœ› Agent: {case.expected_agent.value}")
        print(f"      æœŸæœ›å¤æ‚åº¦ï¼š{case.expected_complexity.value}")
        if case.assertions:
            print(f"      æ–­è¨€æ•°ï¼š{len(case.assertions)}")
        print()


def cmd_report(args):
    """ç”ŸæˆæŠ¥å‘Š"""
    # è¯»å–ä¹‹å‰çš„è¯„æµ‹ç»“æœ
    if not args.input:
        print("âŒ è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶ï¼š--input <result.json>")
        sys.exit(1)

    import json

    try:
        with open(args.input, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.input}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±è´¥ï¼š{e}")
        sys.exit(1)

    # ä»å­—å…¸é‡å»º EvalResult
    from .core.types import EvalResult, CaseResult, AssertionResult, FailureReason, AgentRole, TaskComplexity

    case_results = []
    for cr in data.get("case_results", []):
        assertion_results = [
            AssertionResult(
                assertion_name=ar["assertion_name"],
                passed=ar["passed"],
                weight=ar["weight"],
                failure_reason=ar.get("failure_reason", ""),
            )
            for ar in cr.get("assertion_results", [])
        ]

        failure_reason = None
        if cr.get("failure_reason"):
            try:
                failure_reason = FailureReason(cr["failure_reason"])
            except ValueError:
                pass

        case_results.append(CaseResult(
            case_id=cr["case_id"],
            case_name=cr["case_name"],
            passed=cr["passed"],
            actual_agent=AgentRole(cr["actual_agent"]),
            actual_complexity=TaskComplexity(cr["actual_complexity"]),
            actual_output=cr["actual_output"],
            response_time_ms=cr["response_time_ms"],
            assertion_results=assertion_results,
            failure_reason=failure_reason,
            failure_details=cr.get("failure_details", ""),
            tokens_used=cr.get("tokens_used", 0),
            timestamp=cr.get("timestamp", 0),
        ))

    eval_result = EvalResult(
        run_id=data.get("run_id", ""),
        case_results=case_results,
        start_time=data.get("start_time", 0),
        end_time=data.get("end_time", 0),
        total_cases=data.get("total_cases", 0),
        passed_cases=data.get("passed_cases", 0),
        failed_cases=data.get("failed_cases", 0),
    )

    # ç”ŸæˆæŠ¥å‘Š
    fmt = args.format.lower()

    if fmt == "json":
        reporter = JSONReporter()
        output = args.output or "eval_report.json"
        reporter.export(eval_result, output)
        print(f"âœ… JSON æŠ¥å‘Šå·²ç”Ÿæˆï¼š{output}")

    elif fmt == "html":
        reporter = HTMLReporter()
        output = args.output or "eval_report.html"
        if not output.endswith(".html"):
            output += ".html"
        reporter.export(eval_result, output)
        print(f"âœ… HTML æŠ¥å‘Šå·²ç”Ÿæˆï¼š{output}")

    elif fmt == "console":
        reporter = ConsoleReporter(verbose=True)
        reporter.print(eval_result)

    else:
        print(f"âŒ ä¸æ”¯æŒçš„æŠ¥å‘Šæ ¼å¼ï¼š{fmt}")
        print("æ”¯æŒæ ¼å¼ï¼šjson, html, console")
        sys.exit(1)


def category_name(category: EvalCategory) -> str:
    """è·å–ç±»åˆ«ä¸­æ–‡å"""
    names = {
        EvalCategory.SIMPLE: "ç®€å•ä»»åŠ¡",
        EvalCategory.MEDIUM: "ä¸­ç­‰ä»»åŠ¡",
        EvalCategory.COMPLEX: "å¤æ‚ä»»åŠ¡",
        EvalCategory.EDGE: "è¾¹ç•Œ/å¼‚å¸¸",
    }
    return names.get(category, str(category))


def main():
    parser = argparse.ArgumentParser(
        description="Talker-Thinker è¯„æµ‹ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m evals run                          # è¿è¡Œå…¨éƒ¨è¯„æµ‹
  python -m evals run --category simple        # è¿è¡Œç®€å•ä»»åŠ¡è¯„æµ‹
  python -m evals run --verbose --output result.json
  python -m evals list                         # åˆ—å‡ºæ‰€æœ‰è¯„æµ‹ç”¨ä¾‹
  python -m evals list --category complex
  python -m evals report --input result.json --format html
        """,
    )

    subparsers = parser.add_subparsers(dest="command", help="å‘½ä»¤")

    # run å‘½ä»¤
    run_parser = subparsers.add_parser("run", help="è¿è¡Œè¯„æµ‹")
    run_parser.add_argument(
        "--category", "-c",
        choices=["simple", "medium", "complex", "edge"],
        help="è¿è¡Œç‰¹å®šç±»åˆ«çš„è¯„æµ‹",
    )
    run_parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼)",
    )
    run_parser.add_argument(
        "--html", "-H",
        nargs="?",
        const="eval_report.html",
        help="ç”Ÿæˆ HTML æŠ¥å‘Š (å¯é€‰æŒ‡å®šæ–‡ä»¶å)",
    )
    run_parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†è¾“å‡º",
    )
    run_parser.add_argument(
        "--real-llm",
        action="store_true",
        help="ä½¿ç”¨çœŸå® LLM (é»˜è®¤ä½¿ç”¨ Mock)",
    )
    run_parser.add_argument(
        "--timeout",
        type=float,
        default=60.0,
        help="ç”¨ä¾‹è¶…æ—¶æ—¶é—´ (ç§’)",
    )
    run_parser.add_argument(
        "--latency",
        type=float,
        help="Mock å“åº”å»¶è¿Ÿ (ms)",
    )
    run_parser.set_defaults(func=cmd_run)

    # list å‘½ä»¤
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºè¯„æµ‹ç”¨ä¾‹")
    list_parser.add_argument(
        "--category", "-c",
        choices=["simple", "medium", "complex", "edge"],
        help="åˆ—å‡ºç‰¹å®šç±»åˆ«çš„è¯„æµ‹ç”¨ä¾‹",
    )
    list_parser.set_defaults(func=cmd_list)

    # report å‘½ä»¤
    report_parser = subparsers.add_parser("report", help="ç”ŸæˆæŠ¥å‘Š")
    report_parser.add_argument(
        "--input", "-i",
        required=True,
        help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (JSON æ ¼å¼)",
    )
    report_parser.add_argument(
        "--output", "-o",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„",
    )
    report_parser.add_argument(
        "--format", "-f",
        choices=["json", "html", "console"],
        default="console",
        help="æŠ¥å‘Šæ ¼å¼ (é»˜è®¤ï¼šconsole)",
    )
    report_parser.set_defaults(func=cmd_report)

    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        sys.exit(1)

    args.func(args)


if __name__ == "__main__":
    main()
